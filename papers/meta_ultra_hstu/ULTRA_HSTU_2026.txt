                                             Bending the Scaling Law Curve in Large-Scale
                                             Recommendation Systems
                                             Qin Ding1,∗ , Kevin Course1 , Linjian Ma1 , Jianhui Sun1 , Rouchen Liu1 , Zhao Zhu1 , Chunxing Yin1 , Wei Li1 ,
                                             Dai Li1 , Yu Shi1 , Xuan Cao1 , Ze Yang1 , Han Li1 , Xing Liu1 , Bi Xue1 , Hongwei Li1 , Rui Jian1 , Daisy Shi He1 ,
                                             Jing Qian1 , Matt Ma1 , Qunshu Zhang1 , Rui Li1,∗
                                             1
                                                 Meta Recommendation Systems

                                             Learning from user interaction history through sequential models has become a cornerstone of large-
                                             scale recommender systems. Recent advances in large language models have revealed promising
                                             scaling laws, sparking a surge of research into long-sequence modeling and deeper architectures for
arXiv:2602.16986v1 [cs.IR] 19 Feb 2026




                                             recommendation tasks. However, many recent approaches rely heavily on cross-attention mechanisms
                                             to address the quadratic computational bottleneck in sequential modeling, which can limit the
                                             representational power gained from self-attention. We present ULTRA-HSTU, a novel sequential
                                             recommendation model developed through end-to-end model and system co-design. By innovating
                                             in the design of input sequences, sparse attention mechanisms, and model topology, ULTRA-HSTU
                                             achieves substantial improvements in both model quality and efficiency. Comprehensive benchmarking
                                             demonstrates that ULTRA-HSTU achieves remarkable scaling efficiency gains—over 5× faster training
                                             scaling and 21× faster inference scaling compared to conventional models—while delivering superior
                                             recommendation quality. Our solution is fully deployed at scale, serving billions of users daily and
                                             driving significant 4% to 8% consumption and engagement improvements in real-world production
                                             environments.

                                             Correspondence: qding@meta.com, ruili@meta.com
                                             Date: February 20, 2026




                                         1   Introduction                                                 velopment of Hierarchical Sequential Transduction
                                                                                                          Units (HSTU) (Zhai et al., 2024), which introduces
                                         Recently, transformer-based sequential modeling has              a customized transformer-style architecture designed
                                         emerged as a new paradigm for advancing large-scale              to efficiently learn user interests directly from raw
                                         recommendation research (Kang and McAuley, 2018;                 sequential data. HSTU is notable for being the first
                                         de Souza Pereira Moreira et al., 2021; Zhai et al.,              to demonstrate favorable scaling properties with a
                                         2024; Si et al., 2024) in the era of scaled-GPU compu-           transformer-like approach specifically tailored for
                                         tation, Particularly, traditional deep-learning based            recommendation systems. The sequential model-
                                         recommendation models (DLRM) have focused on                     ing paradigm has since been widely adopted and
                                         feature interactions (Wang et al., 2017) with carefully          further advanced by leading industry practitioners,
                                         engineered human features. While effective, these                including Douyin (Guan et al., 2025; Chai et al.,
                                         models do not scale efficiently with increased com-              2025), Meituan (Han et al., 2025), Alibaba (Wang
                                         pute (Zhai et al., 2024; Wang et al., 2025) on more              et al., 2025), Xiaohongshu (Huang et al., 2025),
                                         feature interactions or additional layers. In contrast,          Meta (Zhai et al., 2024) and Linkedin (Hertel et al.,
                                         transformer-based sequential modeling, which em-                 2024), each contributing their own architectural inno-
                                         phasizes end-to-end learning from raw user behavior              vations. This broad adoption across major platforms
                                         sequences, can jointly capture both long-term prefer-            underscores the effectiveness and impact of sequential
                                         ences and short-term intent (Chen et al., 2019) and              modeling in large-scale recommendation systems.
                                         exhibit favorable scaling laws with compute: model
                                         performance improves with longer sequences, denser               However, transformer-based recommendation models
                                         computation in attention layers, and increased depth             including HSTU suffer from a complexity of O(L2 )
                                         in stacked attention layers.                                     due to the self-attention mechanism, where L de-
                                                                                                          notes the length of the user history sequence. This
                                         A prominent line of research in this area is the de-             quadratic scaling quickly becomes impractical (Za-


                                                                                                      1
heer et al., 2020; Vaswani et al., 2017) when attempt-        To our knowledge, ULTRA-HSTU stands as one of
ing to model user histories containing O(10k) to              the largest sequential models ever deployed in in-
O(100k) events, especially in environments where it           dustry, demonstrating substantially improved scaling
is standard to serve billions of recommendations daily        efficiency. We summarize the technical innovations
with sub-second latency. To mitigate the quadratic            of ULTRA-HSTU as follows.
computational bottleneck, previous approaches from
                                                              Input sequence optimizations. We introduce two com-
industry leaders have primarily adopted cross at-
                                                              plementary designs to optimize original HSTU’s input
tention (Chai et al., 2025; Guan et al., 2025), us-
                                                              sequence processing. First, we effectively merge item
ing only ranking candidates or truncated user histo-
                                                              and action representations in sequence designs and
ries as queries instead of self-attention, which con-
                                                              enhance this simplified design with heterogeneous
siders the entire user history. Alternatively, some
                                                              action encodings. Second, to mitigate inefficiency
methods restrict themselves to shallow architectures,
                                                              caused by cross-rank sequence-length imbalance in
employing only 2–4 attention layers (Guan et al.,
                                                              synchronous distributed training, we propose Load-
2025). These strategies fundamentally diverge from
                                                              Balanced Stochastic Length, enforcing a per-rank
practices in large language models (LLMs). While
                                                              compute-load constraint during stochastic length
these techniques substantially reduce computational
                                                              sampling to reduce stragglers and improve training
complexity, they may forgo the benefits of powerful
                                                              throughput by 15%.
self-attention mechanisms and deeper model archi-
tectures. As demonstrated in our experiments (see             Model-system co-design for extremely efficient atten-
Table 1, 5), self-attention remains superior to cross         tion. We provide an end-to-end model–system co-
attention in industrial settings, especially in terms         design that makes self-attention in HSTU viable for
of enabling stacked layers or scaled up computation.          ultra-long user interaction history modeling in pro-
This distinction represents a key research finding and        duction by eliminating the common quadratic and
highlights a major difference of our work from prior          kernel overheads. On the modeling side, we introduce
solutions: rather than eliminating self-attention, our        a semi-local attention (SLA) mechanism tailored to
work focuses on efficiently harnessing its advantages         the structure of user behavior sequences, achieving
from model and system co-optimizations inspired by            efficient linear sparse attention with O((K1 + K2 ) · L)
DeepSeek-V2 (DeepSeek-AI et al., 2024).                       complexity without sacrificing model quality, where
                                                              K1 and K2 are local and global window sizes respec-
To bend the scaling efficiency of scaled ultra-long
                                                              tively. We show that SLA improves the inference
user history modeling, we introduce ULTRA-HSTU
                                                              scaling efficiency more than 5× compared to baseline
design, the next-generation HSTU model with a com-
                                                              models. On the system side, we pair SLA with fine-
prehensive suite of detailed model and system op-
                                                              tuned, hardware-aware optimizations that remove
timizations inspired by DeepSeek-V2 (DeepSeek-AI
                                                              practical bottlenecks and improve hardware utiliza-
et al., 2024). Here, scaling efficiency is formally
                                                              tion in both training and inference. We co-design a
defined as the slope of the fitted linear regression
                                                              recsys-tailored mixed-precision framework spanning
between model performance and computational cost.
                                                              16/8/4-bit formats: we keep most of the operations
Under fixed input sequence configurations, our opti-
                                                              in BF16 for stability, accelerate the dominant GEMM
mizations achieve more than 21× inference scaling
                                                              computations with FP8, and reduce inference commu-
efficiency and 5× training scaling efficiency relative
                                                              nication traffic with INT4 embedding quantization.
to the original HSTU architecture (Zhai et al., 2024).
                                                              We further extend FlashAttention V3 (Shah et al.,
This advancement effectively bends the scaling curve
                                                              2024) ideas, building custom SLA kernels that han-
of recommendation systems, enabling model quality
                                                              dle HSTU’s SiLU-based attention and non-standard
to accelerate substantially when scaling computa-
                                                              masks, and tuning them for heterogeneous GPU archi-
tional resources (see Figure 1).
                                                              tectures (NVIDIA H100 and AMD MI300) to sustain
To validate the proposed solution, we deployed                high GPU utilization. We also introduce memory sav-
ULTRA-HSTU, with 18 layers of self-attention over             ing optimizations with minimal efficiency overhead
16k user behavior sequences trained with multiple             that significantly cut HBM footprint, enabling ultra-
hundreds of H100 GPUs, in a large-scale production            long sequence training. Together, these co-designed
environment serving billions of users. It achieves sub-       components enable 70% training and 50% inference
stantial consumption and engagement improvements              throughput gains over the same model without these
ranging from 4% to 8% as well as a 0.217% uplift in           system optimizations. Note that to maximize end-to-
topline metrics. This demonstrates both the scaling           end performance, we focus on end-to-end throughput
potential of sequential modeling in the recommenda-           (how fast it finishes training/inference for a fixed num-
tion domain and the effectiveness of our solutions.           ber of examples) given the same model performance,


                                                          2
                                                                 0.8                   HSTU, 9.8e 05 C + 0.013
        0.8
                                                                                       ULTRA-HSTU, 0.0021 C 0.15
        0.6                                                      0.6
 C-NE




                                                          C-NE
        0.4                                                      0.4

        0.2                                                      0.2
                            HSTU, 0.00028 C + 0.032
        0.0                 ULTRA-HSTU, 0.0015 C 0.14            0.0
                250 500 750 1000 1250 1500                             0      1000    2000    3000     4000
                     Train GFLOP / item                                       Inference GFLOP / item
Figure 1 Overall Performance: Scaling performance with respect to train (left) and inference (right) FLOP. Compared
to vanilla HSTU, ULTRA-HSTU has more than 5.3× training scaling efficiency and 21.4× inference scaling efficiency.


instead of purely optimizing GPU utilization.                 progress in recommendation systems has been driven
                                                              by learning from user interaction histories. Deep in-
Dynamic topological model designs. Scalability in rec-
                                                              terest networks (DIN) (Zhou et al., 2018) were one
ommendation models extend beyond sequence length,             of the classic short-sequence learning methods. SAS-
vertical scaling through stacking additional layers           Recs (Kang and McAuley, 2018) is a traditional trans-
yields additional benefits, particularly by increasing        former implementation for recommendations. HSTU
capacity via residual connections (He et al., 2016).          (Zhai et al., 2024) was later proposed to perform
However, naively stacking HSTU with SLA incurs a              better than traditional transformer-based models in
cost of O(DL) where D is the depth of the model.              recommendations with target-aware predictions. By
Building on the insight that different user signals           capturing the implicit and explicit information learnt
yield different predictive values, we propose two novel       from raw user interaction history, HSTU has removed
topological designs to focus computation on most im-          its dependency on human-crafted user-item features
portant signals. Specifically, we propose 1) Attention        and exhibits a favorable scaling law.
Truncation, which runs the first N1 layers on the full
sequence, then selects a shorter valuable segment and
applies additional N2 layers only on that segment;
and 2) Mixture of Transducers (MoT), which pro-               Building on this line of work, our paper focuses on
cesses heterogeneous behavioral signals as multiple           further improving the scaling behavior, aiming to
sequences with separate transducers and fuses their           achieve better models at reduced computational cost.
representations, enabling targeted capacity/compute           Closely related to our work is research focused on
allocation to high-value signals rather than forcing          improving the training and inference efficiency of se-
everything to compete in one timeline. In our experi-         quential models. With notable breakthroughs in na-
ments, both topological designs significantly improve         tive sparse attention (NSA) (Yuan et al., 2025), linear
performance and efficiency tradeoff that further up-          sparse attention has become the focus to deploy scal-
scales our scaling capabilities.                              able big models (Beltagy et al., 2020). In addition to
                                                              exploring sparse attention, Stacked Target-to-History
                                                              Cross Attention (STCA) (Guan et al., 2025) was pro-
2    Related Work                                             posed to only run query focused on ranking targets,
                                                              which significantly reduces the model complexity but
Traditional industrial-scale recommendation models            introduces performance regressions due to the sim-
usually follow the deep learning recommendation               plified attention mechanism without self-attention.
model framework (Naumov et al., 2019; Mudigere                Although STCA achieves linear complexity, more
et al., 2022) that focuses modeling user and item fea-        expensive pre-attention projections were introduced
ture interactions. The past few years have witnessed          in STCA to improve the performance with a big
a paradigm shift in how large-scale recommendation            computational overhead, making it less effective in
models are trained in industry. Rather than rely-             capturing information from shorter sequences (see
ing on cross-user-item features, much of the recent           Table 3 for details).


                                                          3
                                                                                                    ULTRA-HSTU

                                ...                                                                        ...

                                                                          Truncate             STU x
                         Multi-task Module
                                                                                                           ...

                                STU                                                  STU x

                                                                                                           ...
                                STU
                                 ...                                           (d) Attention Truncation
                                STU

                                                                                          +

                       Feature Pre-processor                                                    History   Item features




                                                                                              Candidate Action encoding
                   Sequential         Non- Sequential
                    Features             Features
                                                          (c) Semi-local Attention      (b) Sequence feature preprocessor
                     (a) Recommendation Model




Figure 2 Model design overview: (a) General recommendation model design. (b) Input sequence optimizations with
action-aware designs (c) Semi-local attention mask with linear complexity (d) Attention truncation for dynamic
topological designs.



3    Background                                                beddings via embedding table lookups. For user i,
                                                               we denote its UIH as Xi = {Ii , Ai }, where the item
As illustrated in Figure 2 (a), a typical recommender          embeddings in UIH are Ii = {Ii,j }L    j=1 ∈ R
                                                                                                       i      Li ,d
                                                                                                                    , and
                                                                                                    Li      Li ,d
system takes input features and trains on multi-               action embeddings are Ai = {ai,j }j=1 ∈ R          and Li
task classification problems. Formally, it learns a            is the total length of user i’s UIH. Non-sequential fea-
multi-task model M to output a probability ŷk =               tures include user-side features, such as country, user
M(X, xj ) ∈ [0, 1] for a candidate xj across different         language, etc., and item-side features, such as sparse
prediction tasks yk (e.g., like, video completion, com-        (e.g., raw ID of item) and dense (e.g., click-through
ment), and rank the candidates based on predicted              rate for this item) features. Those user-side features
scores. Here X is the input features of user. We               could be summarized into context-embeddings and
optimize the model by minimizing the cross-entropy             put at the beginning of sequential UIH (Zhai et al.,
loss between predictions ŷk and ground truth labels           2024). Those item-side features could be summarized
yk collected from logs. Throughout the paper, we               as item-embeddings and inserted into sequences as
use L to denote the general sequence length, X to              target-side embeddings (Zhang et al., 2025).
denote the input features. Most generative ranking
paradigm models the input X as a sequence of em-               Model Given a sequence of embeddings, modern rec-
beddings (see context below), and leverage attention           ommender leverages transformer-style models. One
layers to learn probabilities from those sequential            typical arch is Hierarchical Sequential Transduction
embeddings.                                                    Units (HSTU) (Zhai et al., 2024), which shows sig-
                                                               nificant wins on top of vanilla transformers in recom-
                                                               mender systems by the following modifications:
Input As shown in Figure 2 (a), a recommender
uses a feature preprocessor to transfer different input
features into a sequence of embeddings, including:
                                                                               normaliation: X = Norm(Z)                    (1)
user interaction history (UIH) sequence records a se-
quence of items a specific user interacted with and                          pre-attention: U, Q, K, V = ϕ1 (f1 (X)) (2)
the corresponding actions (e.g., like, comment, video                            attention: A = ϕ2 (QK T ) ⊙ M V (3)
                                                                                                                 
completion, etc.) and context (e.g., timestamp). Raw
                                                                           post-attention: Y = f2 (Norm (A) ⊙ U ) (4)
item IDs (and its multi-modal representation), action
types are represented by d-dimensional learnable em-               residual connection: Z = Y + Z                           (5)


                                                          4
Here ⊙ denotes an elementwise product, f1 and f2                              dicted. Thus we mask the action embeddings in all
are MLPs for pre-attention and post-attention projec-                         candidate positions to be ai,j = 0d if j is a candidate
tions respectively, ϕ1 and ϕ2 are SiLU activations. In                        to be ranked in recommender systems. We explored
Equation 3, a causal mask M is applied to maintain                            different ways of combining action and item embed-
the temporal relationship between sequential items.                           dings and choose to simply add the embeddings of
Input embeddings Z are normalized before passing                              items and actions, formulating the sequential input of
to later operations, each layer connects the output                           user i as Xi = {xi,j }L
                                                                                                    j=1 , where xi,j = Ii,j + ai,j . We
                                                                                                     i


from the previous layer Y by standard residual con-                           hypothesis that gradient can be more easily passed
nections (He et al., 2016).                                                   through action encodings in this method. Further,
                                                                              we enhance the construction of action embeddings
While HSTU shows favorable scaling laws for rec-
                                                                              in ULTRA-HSTU by exploring heterogeneous action
ommendation systems, we believe the scaling curve
                                                                              encodings, from both implicit and explicit signals
can be further optimized via a model and system co-
                                                                              and side information through user contextual fea-
design approach similar to Deepseek-V2 (DeepSeek-
                                                                              tures. Importantly, this design reduces the sequence
AI et al., 2024) for LLM. Thus, we introduce ULTRA-
                                                                              length to be half as the UIH designed in the vanilla
HSTU on top of the original HSTU, and the ideas
                                                                              HSTU (Zhai et al., 2024) without sacrificing model
discussed below can be generally applied to other at-
                                                                              quality, allowing ULTRA-HSTU to achieve substan-
tention architectures for sequential recommendation.
                                                                              tial performance improvements while preserving its
                                                                              scalability.
4 ULTRA-HSTU: Extremely Efficient
                                                                              We further designed load-balanced stochastic length
  High-performance Sequential En-                                             algorithm to improve training throughput by 15%. In
  coder                                                                       (Zhai et al., 2024), Stochastic Length (SL) randomly
                                                                              selects users and samples their history sequences to a
                                                                                                                   α
To bend the scaling curve of vanilla HSTU, we make                            predefined threshold length of L 2 during the training
significant improvements over three key areas below:                          stage, where α ∈ (1, 2] is a tunable hyper-parameter
1) input sequence optimization reduces the effective                          of SL. This reduces the computation complexity from
sequence length at the source; 2) a recommender-                              O(L2 ) to O(Lα ) during training and is proved to
tailored sparse attention computation achieves linear                         be able to generalize in inference with the full se-
complexity; 3) dynamic topological design further                             quence length. Similar ideas have been adapted in
enables favorable depth scaling without paying full-                          other papers (Guan et al., 2025) as well. However, in
sequence cost in every layer. Beyond theoretical                              a distributed training environment, this sampling
complexity reductions, ULTRA-HSTU is model and                                process occurs independently on each rank, lead-
hardware co-designed for practical efficiency in large-                       ing to significant variations in the input and out-
scale distributed training and inference recommenda-                          put load (represented by the sum of user sequence
tion settings. Together, we present ULTRA-HSTU                                lengths) on each rank. Such load imbalances could
below with more than 21× inference scaling efficiency                         greatly reduce training efficiency in our synchronous
and 5× training scaling efficiency compared to vanilla                        distributed training framework. We propose load-
HSTU. The general architecture of the model design                            balanced stochastic length (LBSL) algorithm, a vari-
is detailed in Figure 2 and we present each compo-                            ant of SL that explicitly controls per-rank compute
nents in the following sections.                                              to reducePstragglers. Define the load of a rank in a
                                                                              batch as u∈rank nγu , where nu is the sequence length
                                                                              for request u and γ captures the superlinear cost of
4.1     Input Sequences Optimization
                                                                              HSTU (γ ∈ (1, 2)). Load balance is defined as the
We first propose an efficient action encoding method                          ratio between the maximum and minimum rank loads
to effectively shorten the input sequence length                              within a world size. LBSL runs in three stages: it first
by 2× and thus improve the efficiency by 4× in                                performs a short warmup using standard stochastic
attention computation. Recall that the vanilla                                length to estimate a global target load l; then per-
HSTU (Zhai et al., 2024) interleaves items and ac-                            forms constrained sampling that adaptively selects an
tions, formulating the sequence input for user i as                           unsampled set so each rank’s realized load matches l
{Ii,1 , ai,1 , Ii,2 , ai,2 , . . . , Ii,Li , ai,Li }. While this gener-       as closely as possible, while preserving SL’s bias to-
ally supports both retrieval and ranking stages, it                           ward leaving shorter sequences unsampled via weights
causes the sequence in ranking to double the actual                           pu and weighted sampling-without-replacement plus
UIH length. Directly combing actions and items may                            a greedy fill; finally, it applies periodic recalibration of
leak the action information of candidates to be pre-                          l on a configurable interval to track slow shifts in the


                                                                          5
production length distribution. When recalibrated                            
every batch, LBSL matches standard SL’s average                              1
                                                                                   if L − K1 ≤ i + j ≤ L
load but redistributes sampling across ranks (sam-                     Mi,j = 1     if j ≤ K2 and j ≤ L − i       (7)
pling more on heavy-load ranks and less on light-load                        
                                                                              0     otherwise
                                                                             
ranks) to reduce stragglers without harming quality.
Details are in Algorithm 1 in Appendix.                        With this design, the resulting computational com-
                                                               plexity in attention is reduced to linear as O((K1 +
4.2     Model-System Co-Design for Efficiency                  K2 )·L), significantly improving model efficiency when
                                                               sequence length L scales beyond 10k in large-scale
4.2.1   Semi-Local Attention Design                            recommender systems. In contrast to native sparse
                                                               attention (NSA) from DeepSeek (Yuan et al., 2025)
We introduce a novel sparse attention mechanism
                                                               where only local window is applied, we will show in
called Semi-Local Attention (SLA) which achieves
                                                               Section 5 that both the design of local and global win-
linear complexity in attention computation, signifi-
                                                               dows are necessary, which was especially pronounced
cantly improving the inference scaling efficiency of
                                                               in recommenders where users’ long-term behaviors
ULTRA-HSTU by 5×. Recall in Equation 3 that
                                                               are critical.
vanilla HSTU models (Zhai et al., 2024) calculate
full causal self-attention mask with the following for-
mula, incurring quadratic cost when model scales the           4.2.2   System optimizations
sequence length.
                                                               Mixed-Precision Training and Inference Large-scale
                                                             recommendation models are bottlenecked by a com-
        A(X) = ϕ2 Q(X)K(X)T ⊙ M V (X),              (6)
                                                               bination of dense compute including General Matrix
                                                               Multiplications (GEMMs) and data movement, espe-
where M ∈ RL×L is a causal attention mask with                 cially embedding lookup and host-to-device transfer
Mi,j = 1 only when j ≤ L − i, ϕ2 is SiLU in HSTU.              in serving. To make ULTRA-HSTU efficient end-to-
                                                               end, we co-design a recsys-tailored mixed-precision
                                                               framework spanning 16/8/4-bit formats: we keep
                                                               most of the operations in BF16 for stability, accel-
                                                               erate the dominant GEMM computations with FP8,
                                                               and reduce inference communication traffic with INT4
                                                               embedding quantization. In both offline and online
                                                               experiments, this mixed-precision stack yields 10%
                                                               training and 40% serving throughout improvement
                                                               while preserving model accuracy. We develop a cus-
                                                               tomized FP8 stack (shown in Figure 4) for HSTU
Figure 3 Attentions masks. Left plot: full causal self-        that targets two practical bottlenecks simultaneously:
attention masks. Right plot: Semi-local attention masks.       improving Tensor Core utilization on NVIDIA H100
                                                               to raise achieved TFLOP/s on the dense compute,
                                                               and reducing the overhead of FP8 quantization/scal-
In large-scale recommender systems, the length of              ing that can otherwise become memory-bandwidth
UIH quickly accumulates and goes beyond 10k, lead-             bound. Each HSTU layer contains two GEMMs:
ing to an undeployable situation in real-world ranking.        a projection that maps input embeddings X into
Motivated by the intrinsic sparse and dynamic at-              the U, V, Q, K tensors prior to attention, and a post-
tention in both LLM (Yuan et al., 2025) and Recsys,            attention projection that transforms the normalized
we develop a semi-local attention mechanism that               and gated attention output to produce the layer out-
leverages its nature of sparsity, with a focus on both         put. We execute both GEMMs in FP8, while pre-
long-term and local patterns. We define two hyper-             serving all remaining operations in BF16, improv-
parameters, local window size K1 and global window             ing throughput without sacrificing numerical robust-
size K2 . Local window size controls the window                ness. Simply switching GEMMs to FP8 is inefficient:
length of local pattern that will be counted into the          naive FP8 pipelines need an additional operation on
attention mask, and global window size focuses on              scaling/quantization and layout preparation which
the latest UIH attention patterns, capturing users’            can offset the expected speedup. To ensure FP8 ac-
long-term interests. The resulting attention mask is           celerates end-to-end training/inference, we develop
defined as the following in semi-local attention (see          fused kernels that combine row-wise scaling compu-
Figure 3 for illustration):                                    tation and quantization with the preceding layer-


                                                           6
                                                              via Composable Kernel (AMD, 2025). Since MI300x
                                                              lacks H100 features leveraged by FlashAttention-3
                                                              (e.g., TMA and warp-specialized async execution), we
                                                              introduce MI300x-native optimizations: XCD-aware
                                                              scheduling to exploit the 8-chiplet topology, LDS lay-
                                                              outs to reduce shared-memory bank conflicts, and
                                                              explicit VMEM/MFMA interleaving via scheduling
                                                              barriers—and obtain a 2× speedup over the Triton
                                                              kernel baseline.

                                                              Memory Saving with Minimal Overhead The stan-
                                                              dard attention implementation incurs high GPU mem-
                                                              ory pressure in the forward pass, which becomes a pri-
Figure 4 Mixed precision computation framework. We            mary bottleneck for ultra-long sequence training. We
fuse the scaling/quantization steps with the preceding        carefully designed the following optimizations to save
kernels.                                                      memory and preserve training efficiency. First, we
                                                              introduce selective activation rematerialization spe-
                                                              cialized for ULTRA-HSTU. Specifically, we skip sav-
                                                              ing six large forward tensors and reconstruct them in
normalization kernels (Equations 1 and 4) for jagged
                                                              backward with minimal recompute, including reusing
embeddings, eliminating extra passes over memory
                                                              saved layer-norm statistics for normed X, rerunning
and reducing quantization overhead. We further de-
                                                              GEMM to recover U, Q, K, V , and computing the
velop high-performance Triton FP8 GEMM kernels
                                                              intermediate Y inside the fused gated normalization
specifically for the post-attention projection. In this
                                                              kernel. This is substantially lighter than generic
path, the projection output must be accumulated
                                                              checkpointing, with only 5% overhead compared to
with a 2D residual tensor (Equation 5), so we fuse
                                                              the baseline without any activation recomputation.
this residual accumulation directly into the GEMM
                                                              The detailed algorithm is in Listing 1 in Appendix.
epilogue. This is not efficiently supported by Py-
                                                              Second, we remove that overhead by eliminating gra-
Torch GEMM kernels, which typically assume a 1D
                                                              dient concatenation for dU, dQ, dK, dV , cutting mem-
bias vector. Our Triton FP8 kernel natively sup-
                                                              ory traffic and kernel overhead in backward. Overall,
ports 2D bias, while leveraging persistent scheduling,
                                                              ULTRA-HSTU achieves about 67% per-layer memory
TMA, warp specialization, and epilogue pipelining
                                                              reduction with no regression in efficiency. With a 512
to sustain high throughput without excessive register
                                                              embedding dimension size, 256 batch size, and 3k
pressure. In addition to FP8 GEMM, our mixed pre-
                                                              sequence length and BF16 data type, the technique
cision framework incorporates 4-bit quantization for
                                                              reduces the HBM memory usage per layer from 7GB
embedding movement during serving. Further details
                                                              to 2.3GB. Thrid, we employ a fully jagged tensor
are provided in Appendix D.
                                                              implementation for end-to-end training, eliminating
                                                              the need for padding to dense tensors and signifi-
Efficient SLA Kernels for Heterogenous Hardware               cantly reducing memory usage. We present detailed
Attention operations are a bottleneck for HSTU.               efficiency benchmark experiments in Appendix E.
In (Zhai et al., 2024), the kernel is implemented with
Triton (Tillet et al., 2019) using the FlashAttention
                                                              4.3   Dynamic Topological Design
V2 algorithm (Dao, 2024). We improve this baseline
by adopting the FlashAttention-V3 algorithmic de-             In addition to scaling sequence length, depth scaling
sign (Shah et al., 2024) to aggressively overlap data         are crucial for model performance. Naively stacking
movement and compute, while customizing the kernel            ULTRA-HSTU layers with SLA each processing the
to HSTU’s non-standard attention (pointwise SiLU              full sequences incurs a computational cost of O(DL)
activation and SLA masking). We implemented this              where D is the model depth. In real-world applica-
design on both NVIDIA H100 and AMD MI300x, al-                tions, when L scales to 10k sequence lengths, stacking
lowing heterogeneous service and delivery 2× speedup          more and more model layers introduces significant
over the FlashAttention-V2 baseline on both plat-             training, memory and inference costs even when lin-
forms. On NVIDIA H100, we implement CUDA                      ear sparse attention are already in place. However,
kernel families for both full and semi-local HSTU at-         the necessity to be able to handle millions of requests
tention using FlashAttention-3-style pipelining. We           in large-scale recommender systems within millisec-
also implement an analogous kernel on AMD MI300x              onds remains the same. One natural question is that


                                                          7
                  HSTU, 0.00099 C + 0.039                     HSTU, 0.0011 C 0.067                              HSTU, 0.0023 C 0.069
                  HSTU-SLA, 0.0027 C 0.16                     HSTU-SLA, 0.0056 C 0.44                           AT, 0.0079 C 0.31
          0.8                                       0.8
          0.6                                       0.6                                              0.4
   C-NE




                                                                                              C-NE
                                             C-NE
          0.4                                       0.4
                                                                                                     0.2
          0.2                                       0.2
          0.0                                       0.0                                              0.0
                200    400     600     800                   200    400     600    800                         100          200
                Train GFLOP / item                        Inference GFLOP / item                           Inference GFLOP / item
Figure 5 Ablation study on scaling: training (left) and inference (middle) of SLA, inference (right) of attention
truncation (AT).



do we really need attention on full sequences in every                 allocation of computational resources across input
layer when we stack more and more layers? Moti-                        sequences. For instance, the model can assign deeper
vated by this, we propose two efficient topological                    layers and greater capacity to high-value sequences,
designs below to further improve the scaling law of                    while reducing resources for well-understood or less
ULTRA-HSTU.                                                            critical sequences. This targeted allocation of com-
                                                                       putation budget ensures that the model focuses its
Attention Truncation. Motivated by the importance of                   capacity on the most meaningful user interactions,
users’ most recent interaction history, after stacking                 thereby improving overall recommendation quality
N1 layers of HSTU with full sequence length L, we                      and efficiency tradeoff.
propose to select a segment of length L′ from the
full sequence, and stack another N2 layers of HSTU                     Both of the proposed topological designs achieve sig-
on the selected UIH segment only. Many methods                         nificantly better model quality and cost tradeoff than
can be applied to select this UIH segment, including                   vanilla HSTU. The design of Attention Truncation
1) truncating latest UIH of length L′ ; 2) applying                    and MoT are compatible with each other and can
Stochastic Length (SL) (Zhai et al., 2024) on top                      be combined into one model. In real applications,
of the first SL to select sequence length of L′ ; 3)                   the choice of the topological designs depends on the
inserting a compression modules after the first N1                     most concerning metrics (efficiency or model quality)
layers to compress the full sequence to length L′ . In                 in the system. In our experimental setups reported
practice, we found that simply truncating the latest                   below (Section 5), we choose attention truncation
UIH segment gives the best model performance (See                      due to its simplicity and powerful model quality and
Figure 2 (d)).                                                         efficiency tradeoff. We defer the study of MoT to
                                                                       Appendix A.
Mixture of Transducers. Recommendation models in-
herently process multiple input sequences, as user
engagement signals from various sources and types                      5     Experimental Results
are typically recorded separately. Aggregating all
user signals into a single input sequence for a unified                Throughout this section, we measure model quality
encoder compresses heterogeneous user interactions                     by normalized entropy (NE), defined as the model’s
into one timeline can dilute sparse, high-value en-                    cross-entropy divided by the cross-entropy by purely
gagements among dense, implicit signals and force                      making predictions based on mean frequency of posi-
all signals to compete for limited sequence capacity.                  tive labels (He et al., 2014). Formally, NE is defined
To address this challenge, we introduce the Mixture                    in the following equation:
of Transducers (MoT) paradigm. MoT processes                                             PN
multiple distinct input sequences through separate                                − N1    i=1 (yi log pi + (1 − yi ) log(1 − pi ))
                                                                           NE =                                                        ,
transducers and subsequently fuses the learned user                                       −p log p − (1 − p) log(1 − p)
embeddings. This approach enables the model to                                                                               (8)
capture different types of user behaviors over vary-                   where N is the number of training examples, yi ∈
ing time spans, resulting in a more granular and                       {0, 1} is the label for example i, pi is the model predic-
                                                                                                    PN
effective representation of diverse and sparse engage-                 tion for example i, and p = i=1 yi /N . The model is
ment patterns. Crucially, MoT allows for flexible                      better if the NE is lower. Specifically, we measure the


                                                                   8
NE improvement of a consumption task (e.g., video            FLOP on all methods. We observe that ULTRA-
view complete) and an engagement task (e.g., share),         HSTU significantly outperforms all other methods.
denoted as C-NE and E-NE. Here, we choose to report          STCA which heavily relies on cross attention for lin-
NE to follow the original HSTU paper (Zhai et al.,           ear complexity performs worse than ULTRA-HSTU
2024) and the best practices internally (He et al.,          due to lacking the power from self-attention. Note
2014). Based on our experiences and experiments,             that model is worse when ∆ NE is positive. Based
AUC and other metrics move in a consistent direction         on our experience, an improvement in the range of
(better or worse) and at similar scales with NE. We          0.03% − 0.05% is regarded as significant and can lead
omit reporting them due to space limitations.                to substantial gains in online metrics.
We evaluate our model against several strong base-
                                                                      Model              ∆ C-NE    ∆ E-NE
lines, categorized by their ability to model short- or
long-range user behaviors. Short-sequence methods                     ULTRA-HSTU           0%        0%
include DIN (Zhou et al., 2018) and SASRecs (Kang                     HSTU               +0.43%     +0.04%
and McAuley, 2018). Long-sequence methods include                     STCA               +0.94%     +0.74%
vanilla HSTU (Zhai et al., 2024), STCA (Guan et al.,                  Transformer        +0.57%     +0.59%
                                                                      DIN                +1.41%     +1.91%
2025). In addition, we also compare our methods to
                                                                      SASRecs            +1.12%     +1.28%
an internally optimized transformer which has ad-
ditional projections and normalization to stabilize
                                                             Table 1 Model performance on Industrial datasets.
training and avoid unexpected metric regressions in
the classic transformers for recommender systems.
                                                             5.1.3   Scaling Law
5.1     Industrial Dataset Benchmark                         To analyze the scaling behavior, we fix the input
5.1.1   Dataset
                                                             sequence designs for both ULTRA-HSTU and vanilla
                                                             HSTU and report the model performance on C-NE
We first report our model’s performance using                and TFLOP comparisons with improved model archi-
industry-scale production datasets sourced from in-          tecture and topological designs. We vary the number
ternal, large-scale, real-world recommender systems.         of modeling layers from 6 to 18 and sequence length
The dataset consists of a subset of online user in-          L ∈ {3072, 8192, 16384} while fixing model dimen-
teraction histories, totaling over 6 billion samples,        sionality at d = 512. Table 2 reports detailed model
each featuring ultra-long user interaction sequences         performance and TFLOP numbers. As sequence
with lengths ranging from 3, 072 to 16, 384 events.          length and the number of layers increase, we saw
To ensure temporal consistency and prevent future            significantly improved efficiency and C-NE metrics
data leakage, we employ a chronological data split:          from ULTRA-HSTU. In Figure 1, we present the
the initial 85% of the data is allocated for training,       C-NE gain relative to the baseline model as a linear
while the remaining 15% is reserved for evaluation.          regression of computational cost for both ULTRA-
                                                             HSTU and vanilla-HSTU. Notably, by comparing the
Please note that, we use LBSL for all the experiments        slope of the fitted linear function, ULTRA-HSTU
on the industrial dataset. For example, when raw             demonstrates a remarkable advancement in scaling
sequence length in inference is 16, 384, training se-        efficiency, achieving a 5.3× improvement in training
quence length is around 4, 400 after applying LBSL.          scaling efficiency and an outstanding 21.4× enhance-
Based on (Zhai et al., 2024), comparing with models          ment in inference scaling efficiency.
trained with full sequence length, LBSL has mini-
mum NE differences while achieving significant train-
ing speed up. This is the reason that our inference          5.2     Open source dataset
FLOP per example is higher than training FLOP per
                                                             Methods such as ULTRA-HSTU and STCA (Guan
example. For more detailed sequence length compar-
                                                             et al., 2025) are designed for industrial-scale recom-
isons during training and inference, please see Table
                                                             menders where user histories span tens of thousands
6 in Appendix.
                                                             of interactions. To demonstrate the general appli-
                                                             cability of our method beyond these extreme-length
5.1.2   Overall Performance                                  settings, we evaluate on public open-source bench-
                                                             marks on KuaiRand1 with much shorter sequences
Table 1 shows the results on all methods with se-            of length 256. Table 3 shows that our approach still
quence length capped at 3, 072. We tune the model
depth/parameters to allow approximately matched                1 https://kuairand.com/




                                                         9
        Model           Sequence length    # layers      ∆ C-NE        Training TFLOP      Inference TFLOP
                              3072              6         0.0%               0.085                 0.118
        HSTU                  8192             11        −0.34%              0.735                 2.756
                              16384            10        −0.44%              1.584                 4.692
                              3072             14        −0.00%         0.119 (↑ 40.0%)     0.070 (↓ 40.7%)
        ULTRA-HSTU            8192             18        −0.58%         0.414 (↓ 43.7%)     0.337 (↓ 87.8%)
                              16384            18        −0.78%         0.639 (↓ 59.7%)     0.436 (↓ 90.7%)

Table 2 Scaling of ULTRA-HSTU on industrial datasets.



achieves the best NE at the lowest computational              achieves significantly improved scaling compared to
cost in both training and inference stages even under         the vanilla HSTU, with 2.7× training scaling effi-
short-sequence scenarios. STCA struggles to adapt to          ciency and 5.1× inference scaling efficiency. We note
shorter sequences due to the expensive pre-attention          that both local window size K1 and global window
computational overhead.                                       size K2 are necessary in the design of SLA, which sig-
                                                              nificantly differs from NSA (Yuan et al., 2025) where
 Model             Training    Inference      NE              only local sliding window is enabled. Moreover, we
                   TFLOP        TFLOP                         find that global window size K2 is more important
                                                              than local window size K1 . For example, if we set
 STCA                626.49      208.75      0.8689
 Transformer         802.39      267.46      0.8688           K1 = 0 and only enable global window in SLA, we
 SASRec              550.92      176.51      0.8804           observe 0.03% C-NE regression. If we set K2 = 0 and
 DIN                 505.08      168.36      0.8685           only enable local window in SLA, we observe 0.35%
 HSTU                617.78      198.80      0.8676           C-NE regression.
 ULTRA-HSTU          504.41      166.41      0.8626

                                                              5.3.3   Dynamic Topological Design
Table 3 Comparisons on KuaiRand benchmark.
                                                              When stacking more layers in vanilla HSTU, we ob-
                                                              serve significant performance gains but with unaf-
5.3     Ablations on Scaling Studies                          fordable training and inference costs. Figure 8 (right
                                                              figure) plots the scaling curve comparing the perfor-
We first briefly mention the impact from input se-
                                                              mance when stacking HSTU layers with Attention
quence optimizations and then ablate the scaling
                                                              Truncation (AT) and purely stacking HSTU layers
efficiency of SLA and attention truncation by fixing
                                                              with full sequences. This experiments are conducted
the input sequence designs for both vanilla HSTU
                                                              with n1 layers of sequence length 3072 in inference
and ULTRA-HSTU. A detailed description of our
                                                              (around 1110 in training after SL) and n2 layers
approach for analyzing scaling laws is in Appendix F.
                                                              of sequence length 512, with n1 = 3, 6, 9, 12 and
                                                              n2 = 0, 3, 6, 9. Attention truncation are more effec-
5.3.1   Input sequence optimization                           tive when the sequence length is longer. With LBSL
Removing item-action interleaving in input sequence           enabled in model training, efficiency savings from at-
design shrinks sequence length by half and signifi-           tention truncation at sequence length around 1110 is
cantly reduces training FLOP by 32.5% and inference           not significant enough, but we see much better results
FLOP by 63.5% given a UIH sequence with length                in inference (3.4× more effective inference scaling)
3, 072. In the meantime, heterogeneous construction           with longer sequences at length 3072.
of action embeddings brings 0.45% C-NE gain com-
pared to baselines. LBSL achieves 15% speedup in              5.4     Online A/B Testing
the world size of 512, demonstrating its effectiveness
in accelerating the training of large-scale sequential        We showcase the results validated by multiple rigor-
models.                                                       ous 30-day online A/B tests to evaluate the effective-
                                                              ness of ULTRA-HSTU on a large-scale production
                                                              video serving platform that reaches billions of users
5.3.2   Semi-Local Attention (SLA)
                                                              daily. We report three different kinds of online met-
In Figure 8 we plot the C-NE vs the total FLOP with           rics: 1) online consumption metrics (C-metric), such
or without SLA enabled. The SLA enabled model                 as watch time, video completion, etc. 2) online en-


                                                         10
gagement metrics (E-metric), such as likes, comments,           HSTU, with 18 layers of self-attention over 16k user
shares, etc. 3) online topline metrics, such as number          sequences trained on hundreds of H100 GPUs, into a
of visits, daily active users, etc.                             large-scale production environment with significant
                                                                impacts, demonstrating the promising direction of
We upgrade the existing production model from
                                                                scaling up sequential models in recommendation and
vanilla HSTU to ULTRA-HSTU. The results, sum-
                                                                the effectiveness of our proposed innovations.
marized in Table 4, reveal substantial and highly
impressive improvements across key metrics. ULTRA-
HSTU delivers significant 4.11% gains in online con-            7    Acknowledgement
sumption metrics and 2% to 8% gains in engagement
metrics depending on the engagement types. Most                 This work would not be possible without contribu-
notably, we observed remarkable enhancements in                 tions from the collaborators and supports from the
critical “top-line” metrics, which are strong indicators        leaderships as follows (alphabetical order): Hao Lin,
of overall platform health. In our system, even single-         Hong Yan, Jiaqi Zhai, Jie Hua, Shilin Ding, Yu He.
digit percentage improvements in engagement and
consumption are considered major breakthroughs.
Furthermore, increases of 0.05% and 0.01% in Top-               References
line 1 and 2, respectively, are regarded highly sig-
nificant at Meta. Collectively, these results provide           AMD. Composable kernel. https://github.com/ROCm/
compelling evidence for the efficacy and potential of            composable_kernel, 2025.
the proposed ULTRA-HSTU approach. To the best
                                                                Iz Beltagy, Matthew E Peters, and Arman Cohan.
of our knowledge, this is the largest model tested in
                                                                  Longformer: The long-document transformer. arXiv
our recommendation platform, achieving one of the                 preprint arXiv:2004.05150, 2020.
largest impacts in the past few years.
                                                                Zheng Chai, Qin Ren, Xijun Xiao, Huizhi Yang, Bo Han,
                                                                  Sijun Zhang, Di Chen, Hui Lu, Wenlin Zhao, Lele
    User Value              Percentage of gain
                                                                  Yu, et al. Longer: Scaling up long sequence model-
    Online C-Metric 1       4.11%                                 ing in industrial recommenders. In Proceedings of the
    Online E-Metric 2       2.27%                                 Nineteenth ACM Conference on Recommender Systems,
    Online E-Metric 3       8.2%                                  pages 247–256, 2025.
    Online E-Metric 3       4.34%
    Online Top-line 1       0.217%                              Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu
    Online Top-line 2       0.037%                                Ou. Behavior sequence transformer for e-commerce
                                                                  recommendation in alibaba. In Proceedings of the 1st
                                                                  international workshop on deep learning practice for
Table 4 One-month online gain over production baseline.
                                                                  high-dimensional sparse data, pages 1–4, 2019.
                                                                Tri Dao. Flashattention-2: Faster attention with better
                                                                  parallelism and work partitioning. In 12th International
6    Conclusions                                                  Conference on Learning Representations, ICLR 2024,
                                                                  2024.
In this work, we present ULTRA-HSTU, a novel ap-
proach of end-to-end model and system co-design                 Gabriel de Souza Pereira Moreira, Sara Rabhi, Jeong Min
that delivers substantial improvements in scaling effi-           Lee, Ronay Ak, and Even Oldridge. Transformers4rec:
                                                                  Bridging the gap between nlp and sequential/session-
ciency of sequential modeling in the recommendation
                                                                  based recommendation. In Proceedings of the 15th
domain. Our contributions can be summarized as
                                                                 ACM conference on recommender systems, pages 143–
follows: 1) as our key research findings, we show                 153, 2021.
self-attention is still superior to cross-attention and
scaling up computation on attention layers and se-              DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingx-
quence length continue improving model performance;               uan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr,
2) as our key tech innovations, we presented multiple             Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli
                                                                  Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fuli Luo,
modeling and system co-optimizations from LBSL in
                                                                  Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang,
input processing, semi local attention, heterogeneous             Hanwei Xu, Hao Yang, Haowei Zhang, Honghui Ding,
hardware kernel optimization with mixed precision                 Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai,
training/inference, to dynamic model topological de-              Jian Liang, Jianzhong Guo, Jiaqi Ni, Jiashi Li, Jin
sign, and achieved 5× training and 21× inference                  Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai
scaling efficiency. 3) as our key sharing with the                Dong, Kaige Gao, Kang Guan, Lean Wang, Lecong
recommendation industry, we deployed our ULTRA-                   Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang,


                                                           11
  Meng Li, Miaojun Wang, Mingchuan Zhang, Minghua                Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
  Zhang, Minghui Tang, Mingming Li, Ning Tian, Pan-                sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
  pan Huang, Peiyi Wang, Peng Zhang, Qihao Zhu,                    ford, Diego de Las Casas, Lisa Anne Hendricks, Jo-
  Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi              hannes Welbl, Aidan Clark, et al. Training compute-
  Ge, Ruizhe Pan, Runxin Xu, Ruyi Chen, S. S. Li,                  optimal large language models.       arXiv preprint
  Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shao-                arXiv:2203.15556, 2022.
  qing Wu, Shengfeng Ye, Shirong Ma, Shiyu Wang,
  Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng,           Yanhua Huang, Yuqi Chen, Xiong Cao, Rui Yang, Min-
  T. Wang, Tian Pei, Tian Yuan, Tianyu Sun, W. L.                  gliang Qi, Yinghao Zhu, Qingchang Han, Yaowei
  Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng                    Liu, Zhaoyu Liu, Xuefeng Yao, and others. To-
  Liang, Wenjun Gao, Wentao Zhang, X. Q. Li, Xi-                   wards Large-scale Generative Ranking. arXiv preprint
  angyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu,                  arXiv:2505.04180, 2025.
  Xiaohan Wang, Xiaojin Shen, Xiaokang Chen, Xiaosha             Wang-Cheng Kang and Julian McAuley. Self-attentive se-
  Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang,                 quential recommendation. In 2018 IEEE international
  Xin Liu, Xin Xie, Xingkai Yu, Xinnan Song, Xinyi                conference on data mining (ICDM), pages 197–206.
  Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y. Wu,                  IEEE, 2018.
  Y. K. Li, Y. X. Wei, Y. X. Zhu, Yanhong Xu, Yan-
  ping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui              Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B
  Li, Yaohui Wang, Yi Zheng, Yichao Zhang, Yiliang                 Brown, Benjamin Chess, Rewon Child, Scott Gray,
  Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao,              Alec Radford, Jeffrey Wu, and Dario Amodei. Scal-
  Yixin Dong, Yixuan Tan, Yiyuan Liu, Yongji Wang,                 ing laws for neural language models. arXiv preprint
  Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng                   arXiv:2001.08361, 2020.
  Zou, Yukun Zha, Yunxian Ma, Yuting Yan, Yuxiang
  You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha,            Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Zhihao
  Zhe Fu, Zhen Huang, Zhen Zhang, Zhenda Xie, Zhewen               Jia, Andrew Tulloch, Srinivas Sridharan, Xing Liu,
  Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu               Mustafa Ozdal, Jade Nie, Jongsoo Park, et al. Software-
  Zhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li,               hardware co-design for fast and scalable training of deep
  and Ziwei Xie. Deepseek-v2: A strong, economical,                learning recommendation models. In Proceedings of the
  and efficient mixture-of-experts language model, 2024.          49th Annual International Symposium on Computer
  URL https://arxiv.org/abs/2405.04434.                           Architecture, pages 993–1011, 2022.

                                                                 Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael
Lin Guan, Jia-Qi Yang, Zhishan Zhao, Beichuan Zhang,
                                                                  Shi, Jianyu Huang, Narayanan Sundaraman, Jongsoo
  Bo Sun, Xuanyuan Luo, Jinan Ni, Xiaowen Li, Yuhang
                                                                  Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu,
  Qi, Zhifang Fan, et al. Make it long, keep it fast:
                                                                  Alisson G Azzolini, et al. Deep learning recommen-
  End-to-end 10k-sequence modeling at billion scale on
                                                                  dation model for personalization and recommendation
  douyin. arXiv preprint arXiv:2511.06077, 2025.
                                                                  systems. arXiv preprint arXiv:1906.00091, 2019.
Ruidong Han, Bin Yin, Shangyu Chen, He Jiang, Fei
                                                                 Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar,
  Jiang, Xiang Li, Chi Ma, Mincong Huang, Xiaoguang
                                                                   Pradeep Ramani, and Tri Dao. Flashattention-3:
  Li, Chunzhen Jing, et al. Mtgr: Industrial-scale gen-
                                                                   Fast and accurate attention with asynchrony and low-
  erative recommendation framework in meituan. In
                                                                   precision. Advances in Neural Information Processing
 Proceedings of the 34th ACM International Conference
                                                                   Systems, 37:68658–68685, 2024.
 on Information and Knowledge Management, pages
  5731–5738, 2025.                                               Zihua Si, Lin Guan, ZhongXiang Sun, Xiaoxue Zang,
                                                                   Jing Lu, Yiqun Hui, Xingchao Cao, Zeyu Yang, Yichen
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian                  Zheng, Dewei Leng, et al. Twin v2: Scaling ultra-
  Sun. Deep residual learning for image recognition. In            long user behavior sequence modeling for enhanced
 Proceedings of the IEEE conference on computer vision             ctr prediction at kuaishou. In Proceedings of the 33rd
 and pattern recognition, pages 770–778, 2016.                     ACM International Conference on Information and
                                                                   Knowledge Management, pages 4890–4897, 2024.
Xinran He, Junfeng Pan, Ou Jin, Tianbing Xu, Bo Liu,
  Tao Xu, Yanxin Shi, Antoine Atallah, Ralf Herbrich,            Philippe Tillet, Hsiang-Tsung Kung, and David Cox. Tri-
  Stuart Bowers, et al. Practical lessons from predict-            ton: an intermediate language and compiler for tiled
  ing clicks on ads at facebook. In Proceedings of the             neural network computations. In Proceedings of the 3rd
  eighth international workshop on data mining for online          ACM SIGPLAN International Workshop on Machine
  advertising, pages 1–9, 2014.                                    Learning and Programming Languages, pages 10–19,
                                                                   2019.
Lars Hertel, Neil Daftary, Fedor Borisyuk, Aman Gupta,
  and Rahul Mazumder. Efficient user history modeling            Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
  with amortized inference for deep learning recommen-             Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,
  dation models. arXiv preprint arXiv:2412.06924, 2024.            and Illia Polosukhin. Attention is all you need. Ad-


                                                            12
  vances in neural information processing systems, 30,
  2017.
Chunqi Wang, Bingchao Wu, Zheng Chen, Lei Shen,
  Bing Wang, and Xiaoyi Zeng. Scaling Transformers
  for Discriminative Recommendation via Generative
  Pretraining. arXiv preprint arXiv:2506.03699, 2025.
Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang.
  Deep & cross network for ad click predictions. In
 Proceedings of the ADKDD’17, pages 1–7. 2017.
Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo,
  Liang Zhao, Zhengyan Zhang, Zhenda Xie, Yuxing Wei,
  Lean Wang, Zhiping Xiao, et al. Native sparse atten-
  tion: Hardware-aligned and natively trainable sparse
  attention. In Proceedings of the 63rd Annual Meeting of
  the Association for Computational Linguistics (Volume
  1: Long Papers), pages 23078–23097, 2025.
Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey,
 Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip
 Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al.
 Big bird: Transformers for longer sequences. Advances
 in neural information processing systems, 33:17283–
 17297, 2020.
Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui
   Li, Xuan Cao, Leon Gao, Zhaojie Gong, Fangda Gu,
  Jiayuan He, et al. Actions speak louder than words:
   trillion-parameter sequential transducers for genera-
   tive recommendations. In Proceedings of the 41st In-
  ternational Conference on Machine Learning, pages
   58484–58509, 2024.
Zhaoqi Zhang, Haolei Pei, Jun Guo, Tianyu Wang, Yufei
  Feng, Hui Sun, Shaowei Liu, and Aixin Sun. Onetrans:
  Unified feature interaction and sequence modeling with
  one transformer in industrial recommender. arXiv
  preprint arXiv:2510.26104, 2025.
Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan,
 Han Zhu, Xiao Ma, Yanghui Yan, Junqi Jin, Han
 Li, and Kun Gai. Deep interest network for click-
 through rate prediction. In Proceedings of the 24th
 ACM SIGKDD international conference on knowledge
 discovery & data mining, pages 1059–1068, 2018.




                                                            13
A     Selection of Topological Designs                            truncation, we achieve significantly better tradeoff
                                                                  between model quality and computation costs. For
Both attention truncation and MoT achieve signifi-                example, comparing 3-layer vanilla HSTU stacked by
cantly better model quality and cost tradeoff than                another 6-layer attention truncation vs only 6-layer
vanilla HSTU. The design of Mixture of Transducers                vanilla HSTU, attention truncation achieves on par
and Attention Truncation are compatible with each                 C-NE metrics and better E-NE metrics with 3% train
other. In this section, we detail the advantage of                TFLOP savings and 38% inference FLOP savings.
each design. In real applications, the choice of the
topological designs depend on the most concerning
metrics (efficiency or model quality) in the system.              B Diminishing Return of Depth Scal-
Mixture of Transducers (MoT). MoT delivers signifi-                 ing in Cross-Attention
cant NE gains on engagement tasks (E-NE in Table 8)
while achieving competitive training/inference FLOP               We show in Table 5 that self-attention is more pow-
savings. By decoupling heterogeneous signals into                 erful than cross attention in terms of model depth
dedicated modules, MoT mitigates the signal com-                  scaling. With a sequence length around 3072, stack-
petition that occurs when diverse input signals are               ing more layers of cross-attention shows saturated
constrained within a single module with limited se-               model performance with 9 layers, while self-attention
quence length.                                                    exhibits consistently better model quality with in-
                                                                  creased number of layers.
Specifically, we employ two specialized HSTU mod-
ules: one for engagement events and one for consump-
tion events, denoted as E-seq and C-seq in Table 8.               C Algorithm of Load                         Balanced
Each module processes shorter sequences compared
                                                                    Stochastic Length
to its single-HSTU counterpart, yet achieves richer
signal representation through careful sequence com-
position. For instance, the dedicated engagement                      Raw UIH length             3072    8192    16384
module, despite having a shorter sequence, captures
                                                                      Train with SL              1110    2600     4400
significantly richer engagement history by mitigating                 Inference without SL       3072    8192    16384
competition with dense consumption signals. We
further optimize computational efficiency by tailor-              Table 6 Average UIH sequence length in training with SL
ing the compute allocation per module; the shorter                and inference without SL.
sequences result in substantially lighter attention op-
erations, yielding significant FLOP savings during
both training and inference.                                                        Bias-Fused      Bias-Split
                                                                      (m, k & n)       FP8             FP8         BF16
                      # layers    ∆ C-NE     ∆ E-NE                   (1M, 512)          414            236         292
                                                                      (1M, 1K)           734            468         410
                          3        0.00%      0.00%
                                                                      (250K, 512)        414            238         281
    Cross-attention       6        -0.09%     -0.05%
                                                                      (250K, 1K)         721            466         399
                          9        -0.14%     -0.15%
                         12        -0.14%     -0.17%
                                                                  Table 7 Benchmarking FP8 GEMM kernel efficiency with
                          3        0.00%      0.00%               2D bias. Performance is reported in TFLOP/s under
                          6        -0.29%     -0.27%              various m, n, k on NVIDIA H100. Bias-Fused FP8 uses
    Self-attention
                          9        -0.38%     -0.47%              our Triton kernel with native 2D bias support, whereas
                         12        -0.46%     -0.65%              Bias-Split FP8 uses Torch FP8 GEMM plus a separate
                                                                  bias addition since it does not support the 2D bias case.
Table 5 Depth scaling of cross-attention vs self-attention
                                                                  Details of Load Balanced Stochastic Length (LBSL)
                                                                  are listed in Algorithm 1. With LBSL enabled in our
Attention Truncation. In Table 9, we list complimen-
                                                                  experiments in Section 5, the training and inference
tary data of attention truncation with increasing                 sequence length when varying raw sequence length
number of layers of vanilla HSTU only or number                   are listed in Table 6.
of attention truncation layers. With deeper model
layers, we observe significant NE wins in both con-
sumption and engagement tasks, but at the cost                    D     Other System Optimizations
of bigger inference FLOP burden. With attention


                                                             14
      Model Component        Raw Seq Length       ∆ C-NE         ∆ E-NE     Training TFLOP      Inference TFLOP
      MoT vs 3k HSTU     2k C-seq and 1k E-seq     +0.01%        −1.00%        0.138 (↑ 6%)       0.076 (↓ 53%)
      MoT vs 16k HSTU    10k C-seq and 3k E-seq    +0.05%        −0.30%       0.872 (↓ 45%)        1.03 (↓ 69%)

Table 8 Performance of MoT. MoT significantly improves E-NE metrics. (C-seq denotes the consumption sequences
and E-seq denotes the engagement sequences.)

 Model                        Setup                            Train TFLOP      Inference TFLOP     ∆ C-NE        ∆ E-NE
                              3-layer HSTU                         0.0427             0.0561         0.00%        0.00%
                              6-layer HSTU                         0.0851             0.1180         -0.29%       -0.27%
 Vanilla HSTU
                              9-layer HSTU                         0.1284             0.1821         -0.38%       -0.47%
                              12-layer HSTU                        0.1705             0.2438         -0.46%       -0.65%
                              3-layer HSTU + 3-layer AT            0.0626             0.0646         -0.21%       -0.24%
 Attention Truncation (AT)    3-layer HSTU + 6-layer AT            0.0825             0.0729         -0.25%       -0.31%
                              3-layer HSTU + 9-layer AT            0.1020             0.0795         -0.33%       -0.35%

Table 9 Scaling comparison between vanilla HSTU and attention truncation at 3072 sequence length. Model is better
when ∆ NE is negative.


Mix-precision serving In model serving, sparse em-                            Quant-Fused       Quant-Separate
bedding features can dominate host-to-device transfer           Part             FP8                 FP8
time under long sequences. We therefore quantize em-            Pre-attn          3.19X               2.34X
bedding tensors to INT4 and keep them in quantized              Post-attn         1.99X               1.01X
form across the embedding lookup and transfer path,
reducing transfer volume and alleviating the commu-           Table 10 Performance comparison between FP8 GEMM
nication bottleneck. In addition, we use groupwise            with standard quantization versus FP8 GEMM with quan-
INT4 that leverages group-specific scaling factors to         tization fused into preceding kernels, reporting speedup
significantly reducing quality loss compared to a sin-        of each approach relative to BF16.
gle scale per row, while still delivering substantial
throughput gains.
                                                              In Table 7, we benchmark the operation D = AB+C
                                                              where A ∈ Rm×k , B ∈ Rk×n , and C ∈ Rm×n , using
E      Efficiency Benchmarks                                  matrix dimensions (m, k, n) that reflect our model
                                                              workload. In particular, we emphasize large lead-
In this section, we provide benchmarks to test the            ing dimensions m induced by jagged, variable-length
system optimizations presented in Section 4.2.2.              sequences, which dominate the compute cost in prac-
                                                              tice. The results show that fusing the 2D bias di-
                                                              rectly into the FP8 GEMM (Bias-Fused FP8) pro-
E.1     Mixed Precision Benchmarks                            vides up to 1.75× speedup compared to using Torch
FP8 precision efficiency We evaluate the perfor-              FP8 GEMM followed by a separate bias addition
mance impact of FP8 on the GEMMs in both the                  (Bias-Split FP8), motivating our Triton implementa-
pre-attention and post-attention blocks, and sum-             tion for post-attention.
marize the speedups in Table 10. A key difference
between the two blocks is the bias format in GEMM:            Finally, Table 10 reports the speedup of the complete
the pre-attention GEMM uses a 1D bias, while the              pre-attention and post-attention parts when switch-
post-attention GEMM uses a 2D bias. Since FP8                 ing from BF16 to FP8. We observe strong end-to-end
GEMM with 1D bias is already supported by Torch,              gains from two sources: (1) higher-performance FP8
we directly use the Torch kernel for the pre-attention        GEMM kernels (including our 2D-bias Triton ker-
GEMM. In contrast, Torch FP8 GEMM does not                    nel for post-attention), and (2) additional savings
natively support the 2D bias case needed by post-             from fusing quantization into the kernels that pre-
attention; therefore, we develop a customized Triton          cede GEMM, which reduces extra memory traffic
FP8 GEMM kernel with native 2D-bias fusion, and               and kernel launch overhead compared to performing
report its kernel-level efficiency in Table 7.                quantization as a separate step.


                                                         15
Algorithm 1 Load-Balanced Stochastic Length
Require: World size R; warmup steps Twarm ; recalibration interval Trecal ; load exponent γ ∈ (1, 2); SL
      parameter α; SL sampling length ℓSL ; batch size b.
 1: Initialize target load ℓ̄ ← 0 and ℓr ← 0 for all ranks r ∈ {1, . . . , R}
 2: for training step t = 1, 2, . . . do
 3:     for all ranks r ∈ {1, . . . , R} in parallel do
 4:            Receive local batch Br with raw lengths {nu }u∈Br
 5:            Compute ℓr ← ℓr +StandardSL_Loads(Br ; α; γ)                                      ▷ pre-truncation proxy
 6:            if t ≤ Twarm then
 7:                 Apply StandardSL(Br ; α) to truncate examples
 8:            else
 9:               Ur ← ∅; s ← 0                                                      ▷ Ur : untruncated set
10:               Compute weights pu ← SLWeight(nu ; α) for all u ∈ Br
11:               Draw a weighted random permutation π of Br without replacement using pu
12:               for u in order π do
                                     γ                γ
13:                   if s + (nγu − ℓSL ) ≤ ℓ̄ − b · ℓSL then
14:                       Ur ← Ur ∪ {u}; s ← s + (nγu − ℓγSL )
15:                  end if
16:               end for
17:               Keep all u ∈ Ur unsampled
18:               For each u ∈ Br \ Ur , apply the same sampling rule as StandardSL(u; α)
19:           end if
20:      end for
21:      if t = Twarm then
                                                     1
                                                         PR
22:          All-reduce to obtain mean load ℓ̄ ← RTwarm    r=1 ℓr
23:          ℓr ← 0 for all ranks r ∈ {1, . . . , R}
24:      else if (t mod Trecal ) = 0 then
                                                         PR
25:          All-reduce to obtain mean load ℓ̄ ← RT1recal r=1 ℓr
26:          ℓr ← 0 for all ranks r ∈ {1, . . . , R}
27:    end if
28: end for



Int4 quantization efficiency The impact of Int4                  E.2    Attention Kernel Benchmarks
sparse embedding quantization on model serving ef-
ficiency is summarized in Table 11. Applying 4-bit               We present attention-kernel efficiency benchmarks
quantization to sparse embeddings reduces host-to-               in Figure 6, comparing our optimized implementa-
device data-transfer latency by around 40% and in-               tion against the FlashAttention-V2 baseline on both
creases peak queries per second (QPS) by over 20%.               NVIDIA H100 and AMD MI300 GPUs. We evaluate
In addition, we observed negligible differences in on-           two settings: semi-local attention (SLA) and causal
line model accuracy after applying 4-bit quantization.           attention.
                                                                 On H100, for causal attention, the ULTRA implemen-
                                                                 tation sustains over 520 TFLOP/s at a 16K sequence
                                                                 length, delivering a 1.64× speedup over the base-
        Dtype           Latency        Peak QPS                  line. For SLA, across a range of batch sizes and
        Int8              13ms            3.6K
                                                                 sequence lengths, our kernels consistently achieve
        Int4          7.9ms (↓ 40%)   4.4K (↑ 22%)               higher throughput and provide up to a 2.5× speedup.
                                                                 On MI300, Figure 6 reports forward-pass kernel per-
Table 11 Impact of sparse embedding datatypes over model
                                                                 formance. Our ULTRA kernels deliver up to a 1.51×
serving efficiency performances. The latency of embed-
                                                                 speedup over the FlashAttention-V2-based implemen-
ding lookup is measured at 3.5K QPS, and peak QPS
refers to the maximum QPS at a end-to-end latency bud-
                                                                 tation. Relative to ULTRA on H100, ULTRA on
get of 80ms. All results are collected from a single H100        MI300 achieves up to a 0.92× throughput ratio at
host.                                                            16K sequence length under small batch sizes. These
                                                                 results highlight our targeted efforts to enable efficient


                                                            16
     Listing 1 ULTRA-HSTU pseudocode with activation rematerialiation.
1    c l a s s HSTULayerFunction ( t o r c h . a u t o g r a d . F u n c t i o n ) :
2            @staticmethod
3            d e f f o r w a r d ( ctx , x , norm_w , linear_w_1 , gated_norm_w , linear_w_2 ) :
4                  normed_x = norm_forward ( x , norm_w)
5                  u , v , q , k = s i l u _ f o r w a r d (addmm( normed_x , linear_w_1 ) )
6                  attn = attention_forward (q , k , v)
7                  y = gated_norm_forward ( a t t n , u , gated_norm_w )
8                  out = addmm( y , linear_w_2 , b i a s=x )
9                  c t x . save_for_backward ( x , u , a t t n , norm_w , linear_w_1 , gated_norm_w ,
                          linear_w_2 )
10                 r e t u r n out
11           @staticmethod
12           d e f backward ( ctx , dout ) :
13                 x , u , a t t n , norm_w , linear_w_1 , gated_norm_w , linear_w_2 = c t x .
                          saved_tensors
14                 y = gated_norm_forward ( a t t n , u , gated_norm_w ) # rematerialize y
15                 normed_x = norm_forward ( x , norm_w)
16                 u , v , q , k = s i l u _ f o r w a r d (addmm( normed_x , linear_w_1 ) ) # rematerialize u , v
                          , q, k
17                 dy = addmm( dout , linear_w_2 . T)
18                 d_linear_w_2 = addmm( y . T, dout )
19                 dx = dout
20                 dattn , du , d_gated_norm_w = gated_norm_backward ( dy , a t t n , u , gated_norm_w )
21                 dq , dk , dv = a t t e n t i o n _ b a c k w a r d ( dattn , q , k , v )
22                 duqkv = c o n c a t ( du , dq , dk , dv )
23                 uqkv = c o n c a t ( u , q , k , v )
24                 d_addmm = s i l u _ b a c k w a r d ( duqkv , uqkv )
25                 d_normed_x = addmm(d_addmm, linear_w_1 . T)
26                 d_linear_w_1 = addmm( normed_x . T, d_addmm)
27                 dx , d_norm_w = norm_backward ( d_normed_x , x , norm_w)
28                 dx += dout
29                 r e t u r n dx , d_norm_w , d_linear_w_1 , d_gated_norm_w , d_linear_w_2



     AMD inference for large-scale models.                         reason for this assumption is that it allows us to
                                                                   keep α and β linear in log-log space without having
                                                                   to estimate the irreducible error term. As we show
     F    Scaling Laws for ULTRA-HSTU                              below, this assumption also ensures that estimates
                                                                   for scaling improvement are conservative.
     In this section we analyze the compute scaling laws
     for the approaches proposed in the present work. To           Consider the estimated scaling ratio between two
     do so we assume that the NE as a function of the              models,
     compute follows a power-law of the form (Kaplan
     et al., 2020; Hoffmann et al., 2022),                                    βˆ1  β ∗ (1 − L∞ /L1 )  β∗
                                                                                  = 1∗               = 1∗ R,        (11)
                                                                              βˆ2  β2 (1 − L∞ /L2 )   β2
                        L(C) = αC −β ,                  (9)
                                                                   where R is a correction factor to the scaling ratio
     where we have have assumed NE metrics L(C) → 0                estimate. The scaling ratio tells us how much model 1
     as computational budget C → ∞. In general this                improves on model 2’s scaling curve. When L1 < L2
     will cause us to systematically underestimate the true        then R < 1. When model 1 also achieves improved
     scaling law exponent by the factor,                           scaling (which is always true for models with lower
                                                                 loss for the compute regions we consider) our estimate
                             ∗       L∞
                       β̂ = β 1 −                      (10)        for the the scaling ratio is conservative.
                                      L
                                                                   Interpreting scaling law exponent improvements. While
     where β̂ indicates our estimate for the true parameter        improvements to scaling law exponents may appear
     β ∗ and L∞ is the irreducible error on the data. The          modest at first glance, their impact compounds dra-


                                                              17
         (a) Forward, H100                   (b) Forward, H100                      (c) Forward, Mi300




         (d) Forward, H100                   (e) Forward, H100                      (f) Forward, Mi300




     (g) Backward, H100          (h) Backward, H100           (i) Backward, H100           (j) Backward, H100

Figure 6 Performance comparison of ULTRA vs Baseline for both H100 and Mi300. ULTRA uses FlashAttention-V3-
style algorithm, while baseline uses Triton implementation with FlashAttention-V2-style algorithm. (a)(d)(g)(h): full
attention TFLOP/s for forward and backward attention kernels on H100. (b)(e)(i)(j): Speedup of ULTRA SLA over
two baselines on H100: full attention with ULTRA implementation and SLA with baseline implementation. (c)(f):
Speedup of ULTRA SLA over three baselines on Mi300.




                                                         18
                      0.684                         HSTU, 0.69 C 0.0022                               0.684                       HSTU, 0.69 C 0.0017
                                                    ULTRA-HSTU, 0.71 C 0.0068                                                     ULTRA-HSTU, 0.7 C 0.006
                      0.682                                                                           0.682
               C-NE   0.680




                                                                                               C-NE
                                                                                                      0.680

                      0.678                                                                           0.678

                      0.676                                                                           0.676
                                         102            102.2          102.4                                    102.2          102.6       103     103.5
                                        Train log(GFLOP / item)                                               Inference log(GFLOP / item)
Figure 7 Overall Compute Scaling Law: We compare the scaling performance of ULTRA-HSTU against vanilla
HSTU with respect to training FLOP (left) and inference FLOP (right). ULTRA-HSTU achieves 3.09× and 3.52×
improvements in the training and inference compute scaling law exponents, respectively.


                         HSTU, 0.7 C 0.0056                                           HSTU, 0.7 C 0.0059                                         HSTU, 0.73 C 0.0044
                         HSTU-SLA, 0.71 C 0.0078                                      HSTU-SLA, 0.72 C 0.01                                      AT, 0.74 C 0.0079
       0.684                                                           0.684                                                   0.714
       0.682                                                           0.682
                                                                                                                               0.712
C-NE




                                                                C-NE




                                                                                                                        C-NE
       0.680                                                           0.680
       0.678                                                           0.678
                                                                                                                               0.710
       0.676                                                           0.676
                 102    102.2   102.4   102.6   102.8                                 102.2       102.6                                101.7     102       102.2       102.4
                 Train log(GFLOP / item)                                       Inference log(GFLOP / item)                             Inference log(GFLOP / item)

Figure 8 Ablation study on scaling: Semi-Local Attention (SLA) improves the training scaling exponent by 1.39× (left)
and the inference scaling exponent by 1.69× (middle). Attention Truncation Scaling Law: combining self-attention
and attention truncation mechanisms yields a 1.8× improvement in inference compute scaling (right).


matically as compute budgets grow. To see this, con-                                          to the overall scaling improvements. Figure 8 show
sider two models with scaling laws L1 (C) = αC −β1                                            the scaling performance of SLA with respect to train-
and L2 (C) = αC −β2 , where β1 = kβ2 for some im-                                             ing and inference FLOP. Our approach achieves an
provement factor k > 1. To achieve the same loss                                              improvement to the scaling law exponent of 1.39×
with model 2 that model 1 achieves with compute C,                                            and 1.69× with respect to training and inference
we require,                                                                                   FLOP, respectively.
                     C2 = C k ,                 (12)                                          Attention truncation scaling performance. Finally, we
meaning that the compute advantage grows as a                                                 analyze the scaling behavior of our attention trunca-
polynomial with the scaling ratio. For example, a 2×                                          tion approach. While improvements to the scaling
improvement in the scaling exponent implies that the                                          law exponent with respect to training FLOP are mi-
baseline model requires quadratically more compute                                            nor, Figure 8 shows we achieve an improvement to
to match the improved model’s performance.                                                    the inference FLOP exponent of 1.8×.

Overall ULTRA-HSTU scaling performance. In Figure 7,
we plot the fitted compute scaling laws for ULTRA-
HSTU versus HSTU as a function of train and infer-
ence FLOP, respectively. We see that ULTRA-HSTU
improves the scaling exponent by 2.08× compared
to HSTU with respect to training FLOP and 4.59×
with respect to inference FLOP.
Semi-Local attention scaling performance. We next iso-
late the contribution of Semi-Local Attention (SLA)


                                                                                      19
