                                          OneTrans: Unified Feature Interaction and Sequence Modeling
                                               with One Transformer in Industrial Recommender
                                                          Zhaoqi Zhangâˆ—                                                Haolei Peiâˆ—                                      Jun Guoâˆ—
                                              Nanyang Technological University                                       ByteDance                                        ByteDance
                                                        ByteDance                                              Singapore, Singapore                              Singapore, Singapore
                                                   Singapore, Singapore                                      haolei.pei@bytedance.com                          jun.guo@bytedance.com
                                               zhaoqi.zhang@bytedance.com

                                                           Tianyu Wang                                                 Yufei Feng                                        Hui Sun
                                                          ByteDance                                                 ByteDance                                        ByteDance
                                                     Singapore, Singapore                                        Hangzhou, China                                 Hangzhou, China
arXiv:2510.26104v3 [cs.IR] 2 Feb 2026




                                                tianyu.wang01@bytedance.com                                  fengyihui@bytedance.com                        sunhui.sunh@bytedance.com

                                                                                        Shaowei Liuâ€                                          Aixin Sunâ€ 
                                                                                      ByteDance                                 Nanyang Technological University
                                                                                 Singapore, Singapore                                Singapore, Singapore
                                                                          liushaowei.nphard@bytedance.com                             axsun@ntu.edu.sg

                                        Abstract                                                                                ACM Reference Format:
                                        In recommendation systems, scaling up feature-interaction mod-                          Zhaoqi Zhang, Haolei Pei, Jun Guo, Tianyu Wang, Yufei Feng, Hui Sun,
                                                                                                                                Shaowei Liu, and Aixin Sun. 2025. OneTrans: Unified Feature Interaction
                                        ules (e.g., Wukong, RankMixer) or user-behavior sequence modules
                                                                                                                                and Sequence Modeling with One Transformer in Industrial Recommender.
                                        (e.g., LONGER) has achieved notable success. However, these ef-                         In Proceedings of Make sure to enter the correct conference title from your
                                        forts typically proceed on separate tracks, which not only hinders                      rights confirmation email (Conference acronym â€™XX). ACM, New York, NY,
                                        bidirectional information exchange but also prevents unified op-                        USA, 9 pages.
                                        timization and scaling. In this paper, we propose OneTrans, a
                                        unified Transformer backbone that simultaneously performs user-                         1    Introduction
                                        behavior sequence modeling and feature interaction. OneTrans
                                                                                                                                Recommendation systems (RecSys) play a fundamental role in var-
                                        employs a unified tokenizer to convert both sequential and non-
                                                                                                                                ious information services, such as e-commerce [9, 35], streaming
                                        sequential attributes into a single token sequence. The stacked
                                                                                                                                media [2, 20, 28] and social networks [31, 32]. Industrial RecSys
                                        OneTrans blocks share parameters across similar sequential to-
                                                                                                                                generally adopt a cascaded ranking architecture [6, 16, 22]. First, a
                                        kens while assigning token-specific parameters to non-sequential
                                                                                                                                recall stage selects hundreds of candidates from billion-scale cor-
                                        tokens. Through causal attention and cross-request KV caching,
                                                                                                                                pora [13, 36]. Then, a ranking stage scores each candidate and
                                        OneTrans enables precomputation and caching of intermediate
                                                                                                                                returns the top-ğ‘˜ items [11, 27, 28, 32, 37]. Deep Learning Recom-
                                        representations, significantly reducing computational costs during
                                                                                                                                mendation Models (DLRMs) [19] are widely adopted in the ranking
                                        both training and inference. Experimental results on industrial-
                                                                                                                                stage of industrial recommenders.
                                        scale datasets demonstrate that OneTrans scales efficiently with
                                                                                                                                   We focus on the ranking stage in this paper, following the DLRM-
                                        increasing parameters, consistently outperforms strong baselines,
                                                                                                                                style ranking paradigm. For ranking, mainstream approaches iterate
                                        and yields a 5.68% lift in per-user GMV in online A/B tests.
                                                                                                                                on two separate modules: (a) sequence modeling, which encodes
                                                                                                                                user multi-behavior sequences into candidate-aware representa-
                                        CCS Concepts
                                                                                                                                tions using local attention or Transformer encoders [1, 14, 25, 35],
                                        â€¢ Information systems â†’ Information retrieval; Recommender                              and (b) feature interaction, which learns high-order crosses among
                                        systems;                                                                                non-sequential features (e.g., user profile, item profile, and context)
                                                                                                                                via factorization, explicit cross networks, or attention over feature
                                        Keywords                                                                                groups [11, 12, 27, 37]. As shown in Fig. 1(a), these approaches
                                        Recommender System, Ranking Model, Scaling Laws                                         typically encode user behaviors into a compressed sequence rep-
                                        âˆ— These authors contributed equally.                                                    resentation, then concatenate it with non-sequential features and
                                        â€  Corresponding author.                                                                 apply a feature-interaction module to learn higher-order interac-
                                                                                                                                tion; we refer to this design as the encode-then-interaction pipeline.
                                                                                                                                   The success of large language models (LLMs) demonstrates that
                                        This work is licensed under a Creative Commons Attribution 4.0 International License.
                                                                                                                                scaling model size (e.g., parameter size, training data) yields pre-
                                        Conference acronym â€™XX, Woodstock, NY                                                   dictable gains in performance [15], inspiring similar investigations
                                        Â© 2025 Copyright held by the owner/author(s).
                                        ACM ISBN 978-1-4503-XXXX-X/18/06                                                        Accepted at The Web Conference 2026 (WWW 2026). Camera-ready version
                                                                                                                                forthcoming.
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY                                                                                                    Zhang et al.


              Multi-task Tower                             Multi-task Tower
                                                                                              Particularly, cross-candidate and cross-request KV caching [1] re-
                                                                                              duces the time complexity from ğ‘‚ (ğ¶) to ğ‘‚ (1) for sessions with ğ¶
          Feature Interaction Block                                                           candidates, making large-scale OneTrans deployment feasible.
                                                          OneTrans Stack
                                                                                                 In summary, our main contributions are fourfold: (1) Unified
     Compressed Seq                                                                           framework. We present OneTrans, a single Transformer back-
 Sequence Modeling Block
                                                                                              bone for ranking, equipped with a unified tokenizer that encodes
                                                               Tokenizer
                                                                                              sequential and non-sequential features into one token sequence, and
   Sequential Features       Non-Seq Features   Sequential Features        Non-Seq Features
                                                                                              a unified Transformer block that jointly performs sequence modeling
     (a) Conventional Approach                             (b) OneTrans                       and feature interaction. (2) Customization for recommenders.
                                                                                              To bridge the gap between LLMs and RecSys tasks, OneTrans
Figure 1: Architectural comparison. (a) Conventional encode-                                  introduces a mixed parameterization that allocates token-specific
then-interaction pipeline encodes sequential features and                                     parameters to diverse non-sequential tokens while sharing param-
merges non-sequential features before a post-hoc feature                                      eters for all sequential tokens. (3) Efficient training and serving.
interaction block. (b) OneTrans performs joint modeling of                                    We improve efficiency with a pyramid strategy that progressively
both sequential and non-sequential features within a single                                   prunes sequential tokens and a cross-request KV Caching that reuses
OneTrans (Transformer-style) stack.                                                           user-side computations across candidates. In addition, we adopt
                                                                                              LLM optimizations such as FlashAttention, mixed-precision train-
                                                                                              ing, and half-precision inference to further reduce memory and
within RecSys [1, 32, 37]. For feature interaction, Wukong [32]                               compute. (4) Scaling and deployment. OneTrans demonstrates
stacks Factorization Machine blocks with linear compression to                                near log-linear performance gains with increased model size, pro-
capture high-order feature interactions and establishes scaling laws,                         viding evidence of a scaling law in real production data. When
while RankMixer [37] achieves favorable scaling through hardware-                             deployed online, it achieves statistically significant lifts on business
friendly token-mixing with token-specific feed-forward networks                               KPIs while maintaining production-grade latency.
(FFNs). For sequence modeling, LONGER[1] applies causal Trans-
formers to long user histories and shows that scaling depth and
width yields monotonic improvements. Although effective in prac-                              2    Related Work
tice, separating sequence modeling and feature interaction as in-                             Early RecSys like DIN [35] and its session-aware variants (DSIN) [9]
dependent modules introduces two major limitations. First, the                                use local attention to learn candidate-conditioned summaries of
encode-then-interaction pipeline restricts bidirectional information                          user histories, but compress behaviors into fixed-length vectors
flow, limiting how static/context features shape sequence represen-                           per candidate, limiting long-range dependency modeling [34]. Self-
tations [30]. Second, module separation fragments execution and                               attentive methods like SASRec [14], BERT4Rec [25], and BST [4]
increases latency, whereas a single Transformer-style backbone                                eliminate this bottleneck by letting each position attend over the
can reuse LLM optimizations e.g., KV caching, memory-efficient                                full history and improve sample efficiency with bidirectional mask-
attention, and mixed precision, for more effective scaling [11].                              ing. Recently, as scaling laws [15] in RecSys are increasingly ex-
   In this paper, we propose OneTrans, an innovative architec-                                plored, LONGER [1] pushes sequence modeling toward industrial
tural paradigm with a unified Transformer backbone that jointly                               scales by targeting ultra-long behavioral histories with efficient
performs user-behavior sequence modeling and feature interaction.                             attention and serving-friendly designs. However, in mainstream
As shown in Fig. 1(b), OneTrans enables bidirectional informa-                                pipelines these sequence encoders typically remain separate from
tion exchange within the unified backbone. It employs a unified                               the feature-interaction stack, leading to late fusion rather than joint
tokenizer that converts both sequential features (diverse behav-                              optimization with static contextual features [30].
ior sequences) and non-sequential features (static user/item and                                 On the feature-interaction side, early RecSys rely on manually en-
contextual features) into a single token sequence, which is then                              gineered cross-features or automatic multiplicative interaction lay-
processed by a pyramid of stacked OneTrans blocks, a Transformer                              ers. Classical models such as Wide&Deep [5], FM/DeepFM [3, 12],
variant tailored for industrial RecSys. To accommodate the diverse                            and DCN/DCNv2 [26, 27] provide efficient low-order or bounded-
token sources in RecSys, unlike the text-only tokens in LLMs, each                            degree interactions. However, as recent scaling studies observe [32],
OneTrans block adopts a mixed parameterization similar to Hi-                                 once the model stacks enough cross layers, adding more stops help-
Former [11]. Specifically, all sequential tokens (from sequential                             ing: model quality plateaus instead of continuing to improve. To
features) share a single set of Q/K/V and FFN weights, while each                             overcome the rigidity of preset cross forms, attention-based ap-
non-sequential token (from non-sequential features) receives token-                           proaches automatically learn high-order interactions. AutoInt [24]
specific parameters to preserve its distinct semantics.                                       learns arbitrary-order relations, and HiFormer [11] introduces group-
   Unlike conventional encode-then-interaction frameworks, One-                               specific projections to better capture heterogeneous, asymmet-
Trans eliminates the architectural barrier between sequential and                             ric interactions. With scaling up increasingly applied to feature-
non-sequential features through a unified causal Transformer back-                            interaction modules, large-scale systems such as Wukong [32] demon-
bone. This formulation brings RecSys scaling in line with LLM                                 strate predictable gains by stacking FM-style interaction blocks with
practices: the entire model can be scaled by adjusting backbone                               linear compression, while RankMixer [37] achieves favorable scal-
depth and width, while seamlessly inheriting mature LLM optimiza-                             ing via parallel token mixing and sparse MoE under strict latency
tions, such as FlashAttention [7], and mixed precision training [17].                         budgets. However, these interaction modules typically adhere to the
OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender                        Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY


                                                                           Task Tower                                                           Mix FFN
                                                                                                                   Output



                                                                                                                      +
    OneTrans Pyramid Stack                                           OneTrans Block x N                                                                FFN      FFN                 FFN
                                                                                                                  Mix FFN



                                                                                                                 RMSNorm
                                                    OneTrans Block x N

                                                                                                                                                 Mix Causal Attention
                                                                                                                      +

                                    OneTrans Block x N                                                              Mix
                                                                                               Pos Emb
                                                                                                               Causal Attention
                                                                                                                                                             Multi Head Attention

                 SEP                          SEP
                                                                                                                 RMSNorm                              QKV     QKV               QKV
                                                          S tokens                 NS tokens
                       Sequential Tokenizer                              Non-Seq Tokenizer

                                                                                                  xN
                                                                                                                 NS/S tokens



                                 (a) OneTrans Framework                                                    (b) OneTrans Block                             (c) Mix Parameterization


Figure 2: System Architecture. (a) OneTrans overview. Sequential (S, blue) and non-sequential (NS, orange) features are
tokenized separately. After inserting [SEP] between user behavior sequences, the unified token sequence is fed into stacked
OneTrans Pyramid Blocks that progressively shrink the token length until it matches the number of NS tokens. (b) OneTrans
Block: a causal pre-norm Transformer Block with RMSNorm, Mixed Causal Attention and Mixed FFN. (c) â€œMixedâ€ = mixed
parameterization: S tokens share one set of QKV/FFN weights, while each NS token receives its own token-specific QKV/FFN.


interaction paradigm, which pushes interactions to a separate stage                                      3.1       OneTrans Framework Overview
and blocks unified optimization with user sequence modeling [30].                                        As illustrated in Fig. 2(a), OneTrans employs a unified tokenizer that
   To date, progress in RecSys has largely advanced along two                                            maps sequential features S to S-tokens, and non-sequential features
independent tracks: sequence modeling and feature interaction.                                           N S to NS-tokens. A pyramid-stacked Transformer then consumes
InterFormer [30] attempts to bridge this gap through a summary-                                          the unified token sequence jointly within a single computation
based bidirectional cross architecture that enables mutual signal                                        graph. We denote the initial token sequence as
exchange between the two components. However, it still maintains
                                                                                                                     X (0) = S-tokens ; NS-tokens âˆˆ R (ğ¿S +ğ¿NS ) Ã—ğ‘‘ .
                                                                                                                                                 
them as separate modules, and the cross architecture introduces                                                                                                              (3)
both architectural complexity and fragmented execution. Without
a unified backbone for joint modeling and optimization, scaling the                                      This token sequence is constructed by concatenating ğ¿S number
system as an integrated whole remains challenging.                                                       of S-tokens and ğ¿NS number of NS-tokens, with all tokens having
   Recent work on Generative Recommenders (GRs) frames rec-                                              dimensionality ğ‘‘. Note that, the S-tokens contain learnable [SEP]
ommendation as sequential transduction and proposes efficient                                            tokens inserted to delimit boundaries between different kind of
long-context backbones such as HSTU [31]. This line is comple-                                           user-behavior sequences. As shown in Fig. 2(b), each OneTrans
mentary to DLRMs that rely on rich non-sequential (NS) features.                                         block progressively refines the token states through:
                                                                                                                                                     
3    Methodology                                                                                                   Z (ğ‘›) = MixedMHA Norm X (ğ‘›âˆ’1) + X (ğ‘›âˆ’1) ,            (4)
Before detailing our method, we briefly describe the task setting.                                                                                 
                                                                                                                   X (ğ‘›) = MixedFFN Norm Z (ğ‘›) + Z (ğ‘›) .
                                                                                                                                                  
In a cascaded industrial RecSys, each time the recall stage returns a                                                                                                   (5)
candidate set (typically hundreds of candidate items) for a user ğ‘¢.
The ranking model then predicts a score to each candidate item ğ‘–:                                        Here, MixedMHA (Mixed Multi-Head Attention) and MixedFFN
                                                                                                         (Mixed Feed-Forward Network) adopt a mixed parameterization
                       ğ‘¦Ë†ğ‘¢,ğ‘– = ğ‘“ ğ‘– N S, S; Î˜
                                             
                                                                  (1)                                    strategy (see Fig. 2(c)) sharing weights across sequential tokens,
where N S is a set of non-sequential features derived from the                                           while assigning separate parameters to non-sequential tokens in
user, the candidate item, and the context; S is a set of historical                                      both the attention and feed-forward layers.
behavior sequences from the user; and Î˜ are trainable parameters.                                           A unified causal mask enforces autoregressive constraints, re-
Common task predictions include the click-through rate (CTR) and                                         stricting each position to attend only to preceding tokens. Specifi-
the post-click conversion rate (CVR).                                                                    cally, NS-tokens are permitted to attend over the entire history of
                                                                                                         S-tokens, thereby enabling comprehensive cross-token interaction.
            CTRğ‘¢,ğ‘– = ğ‘ƒ click = 1 N S, S; Î˜ ,
                                           
                                                                (2)                                      By stacking such blocks with pyramid-style tail truncation applied
            CVRğ‘¢,ğ‘– = ğ‘ƒ conv = 1 click = 1, N S, S; Î˜ .
                                                     
                                                                                                         to S-tokens, the model progressively distills compact high-order
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY                                                                                      Zhang et al.


information into the NS-tokens. The final token states are then             convert its all event eğ‘– ğ‘— as a common dimensionality ğ‘‘:
passed to task-specific heads for prediction.
                                                                                        SÌƒğ‘– = MLPğ‘– (eğ‘–1 ), . . . , MLPğ‘– (eğ‘–ğ¿ğ‘– ) âˆˆ Rğ¿ğ‘– Ã—ğ‘‘ .
                                                                                                                              
                                                                                                                                                     (9)
   By unifying non-sequential and sequential features into a uni-
fied token sequence and modeling them with a causal Transformer,            Aligned sequences SÌƒğ‘– are merged into a single token sequence by
OneTrans departs from the conventional encode-then-interaction              one of two rules: 1) Timestamp-aware: interleave all events by time,
pipeline. This unified design naturally enables (i) intra-sequence          with sequence-type indicators; 2) Timestamp-agnostic: concatenate
interactions within each behavior sequence, (ii) cross-sequence in-         sequences by event impact, e.g., purchase â†’ add-to-cart â†’ click,
teractions across multiple sequences, (iii) multi-source feature inter-     inserting learnable [SEP] tokens between sequences. In the latter,
actions among item, user, and contextual features, and (iv) sequence-       behaviors with higher user intent are placed earlier in the sequence.
feature interactions, all within a single Transformer stack.                Ablation results indicate that, when timestamps are available, the
   The unified formulation enables us to seamlessly inherit mature          timestamp-aware rule outperforms the impact-ordered alternative.
LLM engineering optimizations, including KV caching and memory-             Formally, we have:
efficient attention, thereby substantially reducing inference latency.                                                           ğ‘›
                                                                                                                                âˆ‘ï¸
                                                                              S-Tokens = Merge SÌƒ1, . . . , SÌƒğ‘› âˆˆ Rğ¿ğ‘† Ã—ğ‘‘ , ğ¿ğ‘† =
                                                                                                               
We argue this unified formulation is well suited to tackling multi-                                                                ğ¿ğ‘– + ğ¿SEP . (10)
sequence and cross-domain recommendation challenges in a single,                                                                  ğ‘–=1
and scalable architecture. Next, we detail the design.
                                                                            3.3    OneTrans Block
3.2     Features and Tokenization                                           As shown in Fig. 2(b), each OneTrans block is a pre-norm causal
To construct the initial token sequence X (0) , OneTrans first applies      Transformer applied to a normalized token sequence: ğ¿ğ‘† sequential
a feature preprocessing pipeline that maps all raw feature inputs           S-tokens, followed by ğ¿ğ‘ ğ‘† non-sequential NS-tokens. Inspired by
into embedding vectors. These embeddings are then partitioned into          the findings on heterogeneous feature groups [11], we make a light-
(i) a multi-behavior sequential subset and (ii) a non-sequential subset     weight modification to Transformer to allow a mixed parameter
representing user, item, or context features. Separate tokenizers are       scheme, see Fig. 2(c). Specifically, homogeneous S-tokens share
applied to each subset.                                                     one set of parameters. The NS-tokens, being heterogeneous across
                                                                            sources/semantics, receive token-specific parameters.
3.2.1 Non-Sequential Tokenization. Non-sequential features N S                 Unlike LLM inputs, the token sequence in RecSys combines se-
include both numerical inputs (e.g., price, CTR) and categorical in-        quential S-tokens with diverse NS-tokens whose value ranges and
puts (e.g., user ID, item category). All features are either bucketized     statistics differ substantially. Post-norm setups can cause atten-
or one-hot encoded and then embedded. Since industrial systems              tion collapse and training instability due to these discrepancies. To
typically involve hundreds of features with varying importance,             prevent this, we apply RMSNorm [33] as pre-norm to all tokens,
there are two options for controlling the number of non-sequential          aligning scales across token types and stabilizing optimization.
tokens, denoted by ğ¿ğ‘ ğ‘† :
                                                                            3.3.1 Mixed (shared/token-specific) Causal Attention. OneTrans
Group-wise Tokenizer (aligned with RankMixer [37]). Features                adopts a standard multi-head attention (MHA) with a causal at-
are manually partitioned into semantic groups {g1, . . . , gğ¿ğ‘ ğ‘† }. Each    tention mask; the only change is how Q/K/V are parameterized.
group is concatenated and passed through a group-specific MLP:              Let xğ‘– âˆˆ Rğ‘‘ be the ğ‘–-th token. To compute Q/K/V, we use a shared
              
 NS-tokens = MLP1 (concat(g1 )), . . . , MLPğ¿ğ‘ ğ‘† (concat(gğ¿ğ‘ ğ‘† )) .
                                                                           projection for S-tokens (ğ‘– â‰¤ ğ¿ğ‘† ) and ğ¿ğ‘ ğ‘† token-specific projections
                                                                      (6)   for NS-tokens (ğ‘– > ğ¿ğ‘† ):
                                                                                                            ğ‘„                    
Auto-Split Tokenizer. Alternatively, all features are concatenated                          qğ‘– , kğ‘– , vğ‘– = Wğ‘– xğ‘– , Wğ‘–ğ¾ xğ‘– , Wğ‘‰ğ‘– xğ‘– ,          (11)
and projected once by a single MLP, then split:                             where Wğ‘–Î¨ (Î¨ âˆˆ {ğ‘„, ğ¾, ğ‘‰ }) follows a mixed parameterization scheme:
                                                      
          NS-tokens = split MLP(concat(N S)), ğ¿ğ‘ ğ‘† .            (7)                    ï£² WSÎ¨ ,    ğ‘– â‰¤ ğ¿ğ‘† (shared for S-tokens),
                                                                                       ï£±
                                                                                       ï£´
                                                                              Wğ‘–Î¨ =
                                                                                       ï£´
                                                                                                                                             (12)
                                                                                           Î¨ , ğ‘– >ğ¿
                                                                                       ï£´ WNS,ğ‘–             (token-specific for NS-tokens).
Auto-Split Tokenizer reduces kernel launch overhead compared                           ï£´
                                                                                                       ğ‘†
                                                                                       ï£³
with Group-wise approach, by using a single dense projection. We
                                                                               Attention uses a standard causal mask, with NS-tokens placed
will evaluate both choices through experiments.
                                                                            after S-tokens. This induces: (1) S-side. Each S-token attends only
  Ultimately, non-sequential tokenization yields ğ¿ğ‘ ğ‘† number of
                                                                            to earlier ğ‘† positions. For timestamp-aware sequences, every event
non-sequential tokens, each of dimensionality ğ‘‘.
                                                                            conditions on its history; for timestamp-agnostic sequences (or-
3.2.2 Sequential Tokenization. OneTrans accepts multi-behavior              dered by intent, e.g., purchase â†’ add-to-cart â†’ click/impression),
sequences as                                                                causal masking lets high-intent signals inform and filter later low-
                                                                          intent behaviors. (2) NS-side. Every NS-token attends to the entire
             S = {S1, . . . , Sğ‘› }, Sğ‘– = eğ‘–1, . . . , eğ‘–ğ¿ğ‘– . (8)
                                                                            ğ‘† history, effectively a target-attention aggregation of sequence
Each sequence Sğ‘– consists of ğ¿ğ‘– number of event embeddings e,               evidence, and to preceding NS-tokens, increasing token-level inter-
which is constructed by concatenating the item ID with its corre-           action diversity. (3) Pyramid support. On both S and NS sides,
sponding side information like item category and price.                     causal masking progressively concentrates information toward later
  Multi-behavior sequences can vary in their raw dimensionality.            positions, naturally supporting the pyramid schedule that prunes
Hence, for each sequence Sğ‘– , we use one shared projection MLPğ‘– to          tokens layer by layer, to be detailed shortly.
OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender   Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY


3.3.2 Mixed (shared/token-specific) FFN. Similarly, the feed-forward                     Table 1: Dataset overview for OneTrans experiments.
network follows the same parameterization strategy: token-specific
FFNs for NS-tokens, and a shared FFN for S-tokens,                                             Metric                                               Value
                    MixedFFN(xğ‘– ) = Wğ‘–2 ğœ™ (Wğ‘–1 xğ‘– ).                     (13)                  # Impressions (samples)                           29.1B
                                                                                               # Users (unique)                                 27.9M
Here Wğ‘–1 and Wğ‘–2 follow the mixed parameterization of Eqn. (12),
                                                                                               # Items (unique)                                 10.2M
i.e., shared for ğ‘– â‰¤ ğ¿ğ‘† and token-specific for ğ‘– > ğ¿ğ‘† .
                                                                                               Daily impressions (mean Â± std)          118.2M Â± 14.3M
   In summary, relative to a standard causal Transformer, One-
                                                                                               Daily active users (mean Â± std)            2.3M Â± 0.3M
Trans changes only the parameterization: NS-tokens use token-
specific QKV and FFN; S-tokens share a single set of parameters. A
single causal mask ties the sequence together, allowing NS-tokens
to aggregate the entire behavior history while preserving efficient,                    Since user behavioral sequences are append-only, we extend KV
Transformer-style computation.                                                       Caching across requests: each new request reuses the previous cache
                                                                                     and computes only the incremental keys/values for newly added
3.4     Pyramid Stack                                                                behaviors. This reduces per-request sequence computation from
                                                                                     ğ‘‚ (ğ¿) to ğ‘‚ (Î”ğ¿), where Î”ğ¿ is the number of new behaviors since
As noted in Section 3.3, causal masking concentrates information
                                                                                     the last request.
toward later positions. Exploiting this recency structure, we adopt
a pyramid schedule: at each OneTrans block layer, only a subset of                   3.5.2 Unified LLM Optimizations. We employ FlashAttention-2 [8]
the most recent S-tokens issue queries, while keys/values are still                  to reduce attention I/O and the quadratic activation footprint of
computed over the full sequence; the query set shrinks with depth.                   vanilla attention via tiling and kernel fusion, yielding lower mem-
                ğ¿ be the input token list and Q = {ğ¿âˆ’ğ¿ â€² +1, . . . , ğ¿}
   Let X = {xğ‘– }ğ‘–=1                                                                  ory usage and higher throughput in both training and inference.
denote a tail index set with ğ¿ â€² â‰¤ ğ¿. Following Eqn. 12, we modify                   To further ease memory pressure, we use mixed-precision train-
queries as ğ‘– âˆˆ Q:                                                                    ing (BF16/FP16) [18] together with activation recomputation [10],
                                  ğ‘„                                                  which discards selected forward activations and recomputes them
                        qğ‘– = Wğ‘– xğ‘– ,          ğ‘– âˆˆ Q,                     (14)        during backpropagation. This combination trades modest extra
while keys and values are computed as usual over the full sequence                   compute for substantial memory savings, enabling larger batches
{1, . . . , ğ¿}. After attention, only outputs for ğ‘– âˆˆ Q are retained,                and deeper models without architectural changes.
reducing the token length to ğ¿ â€² and forming a pyramidal hierarchy
across layers.                                                                       4     Experiments
   This design yields two benefits: (i) Progressive distillation: long be-           Through both offline evaluations and online tests, we aim to an-
havioral histories are funneled into a small tail of queries, focusing               swer the following Research Questions (RQs): RQ1: Unified stack
capacity on the most informative events and consolidating infor-                     vs. encodeâ€“thenâ€“interaction. Does the single Transformer stack
mation into the NS-tokens;       and (ii) Compute efficiency: attention              yield consistent performance gains under the comparable compute?
cost becomes ğ‘‚ ğ¿ğ¿ â€²ğ‘‘ and FFN scales linearly with ğ¿ â€² . Shrinking
                          
                                                                                     RQ2: Which design choices matter? We conduct ablations on
the query set directly reduces FLOPs and activation memory.                          the input layer (e.g., tokenizer, sequence fusion) and the OneTrans
                                                                                     block (e.g., parameter sharing, attention type, pyramid stacking) to
3.5     Training and Deployment Optimization                                         evaluate the importance of different design choices for performance
3.5.1 Cross Request KV Caching. In industrial RecSys, samples                        and efficiency. RQ3: Systems efficiency. Do pyramid stacking,
from the same request are processed contiguously both during                         cross-request KV Caching, FlashAttention-2, and mixed precision
training and serving: their S-tokens remain identical across candi-                  with recomputation reduce FLOPs/memory and latency under the
dates, while NS-tokens vary per candidate item. Leveraging this                      same OneTrans graph? RQ4: Scaling law. As we scale length
structure, we integrate the widely adopted KV Caching [1] into                       (token sequence length), width (ğ‘‘ model ), depth (number of layers),
OneTrans, yielding a unified two-stage paradigm.                                     do loss/performance exhibit the expected log-linear trend? RQ5:
                                                                                     Online A/B Tests. Does deploying OneTrans online yield statisti-
Stage I (S-side, once per request). Process all S-tokens with causal                 cally significant lifts in key business metrics (e.g., order/u, GMV/u)
masking and cache their key/value pairs and attention outputs. This                  under production latency constraints?
stage executes once per request.
Stage II (NS-side, per candidate). For each candidate, compute                       4.1      Experimental Setup
its NS-tokens and perform cross-attention against the cached S-                      4.1.1 Dataset. For offline evaluation, we evaluate OneTrans in a
side keys/values, followed by token-specific FFN layers. Specially,                  large-scale industrial ranking scenario using production logs under
candidate-specific sequences (e.g., SIM [21]) are pre-aggregated                     strict privacy compliance (all personally identifiable information
into NS-tokens via pooling, as they cannot reuse the shared S-side                   is anonymized and hashed). Data are split chronologically, with
cache.                                                                               all features snapshotted at impression time to prevent temporal
   The KV Caching amortizes S-side computation across candidates,                    leakage and ensure online-offline consistency. Labels (e.g., clicks
keeping per-candidate work lightweight and eliminating redundant                     and orders) are aggregated within fixed windows aligned with
computations for substantial throughput gains.                                       production settings. Table 1 summarizes the dataset statistics.
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY                                                                              Zhang et al.


Table 2: Offline effectiveness (CTR/CVR) and efficiency; higher AUC/UAUC is better. * indicates models deployed in our
production in chronological order: DCNv2+DIN â†’ RankMixer+DIN â†’ RankMixer+Transformer â†’ OneTransS â†’ OneTransL

                                                                           CTR              CVR (order)              Efficiency
         Type                             Model
                                                                     AUC â†‘      UAUC â†‘    AUC â†‘     UAUC â†‘     Params (M)     TFLOPs
         (1) Base model                   DCNv2 + DIN (base)*        0.79623    0.71927   0.90361   0.71955              10       0.06
                                          Wukong + DIN               +0.08%     +0.11%    +0.14%    +0.11%              28        0.54
         (2) Feature-interaction          HiFormer + DIN             +0.11%     +0.18%    +0.23%    -0.20%             108        1.35
                                          RankMixer + DIN*           +0.27%     +0.36%    +0.43%    +0.19%             107        1.31
                                          RankMixer + StackDIN       +0.40%     +0.37%    +0.63%    -1.28%             108        1.43
         (3) Sequence-modeling            RankMixer + LONGER         +0.49%     +0.59%    +0.47%    +0.44%             109        1.87
                                          RankMixer + Transformer*   +0.57%     +0.90%    +0.52%    +0.75%             109        2.51
                                          OneTransS *                +1.13%     +1.77%    +0.90%    +1.66%              91        2.64
         (4) Unified framework
                                          OneTransL (default)*       +1.53%     +2.79%    +1.14%    +3.23%             330        8.62


4.1.2 Tasks and Metrics. We evaluate two binary ranking tasks             clipping thresholds of 90 for dense layers and 120 for sparse lay-
as defined in Eqn. (2): CTR and CVR. Performance is measured by           ers. For online inference, we adopt a smaller batch size of 100 per
AUC and UAUC (impression-weighted user-level AUC).                        GPU to balance throughput and latency. Training uses data-parallel
   Next-batch evaluation. Data are processed chronologically.             all-reduce on 16 H100 GPUs.
For each mini-batch, we (i) log predictions in eval mode, then (ii)
train on the same batch. AUC and UAUC are computed daily from             4.2     RQ1: Performance Evaluation
each dayâ€™s predictions and finally macro-averaged across days.
                                                                          We anchor our comparison on DCNv2+DIN, the pre-scaling pro-
   Efficiency metrics. We report Params (model parameters ex-
                                                                          duction baseline in our scenario (Table 2). Under the encode-then-
cluding sparse embeddings) and TFLOPs (training compute in TFLOPs
                                                                          interaction paradigm, scaling either component independently is
at batch size 2048).
                                                                          beneficial: upgrading the feature interaction module (DCNv2 â†’
4.1.3 Baselines. We construct industry-standard model combi-              Wukong â†’ HiFormer â†’ RankMixer) or the sequence modeling
nations as baselines using the same features and matched com-             module (StackDIN â†’ Transformer â†’ LONGER) yields consistent
pute budgets. Under the encode-then-interaction paradigm, we start        gains in CTR AUC/UAUC and CVR AUC. In our system, improve-
from the widely-used production baseline DCNv2+DIN [27, 35]               ments above +0.1% in these metrics are considered meaningful,
and progressively strengthen the feature-interaction module:              while gains above +0.3% typically correspond to statistically sig-
DCNv2 â†’ Wukong [32] â†’ HiFormer [11] â†’ RankMixer [37]. With                nificant effects in online A/B tests. However, CVR UAUC is treated
RankMixer fixed, we then vary the sequence-modeling module:               cautiously due to smaller per-user sample sizes and higher volatility.
StackDIN â†’ Transformer [4] â†’ LONGER [1].                                     Moving to a unified design, OneTransS surpasses the baseline
                                                                          by +1.13%/+1.77% (CTR AUC/UAUC) and +0.90%/+1.66% (CVR
4.1.4 Hyperparameter Settings. We report two settings: OneTransS
                                                                          AUC/UAUC). At a comparable parameter scale, it also outperforms
uses 6 stacked OneTrans blocks width ğ‘‘=256, and ğ» =4 heads, tar-
                                                                          RankMixer+Transformer with similar training FLOPs (2.64T vs.
geting â‰ˆ 100M parameters. OneTransL scales to 8 layers with
                                                                          2.51T), demonstrating the benefits of unified modeling. Scaling fur-
width ğ‘‘=384.
                                                                          ther, OneTransL delivers the best overall improvement of +1.53%
   Inputs are processed through a unified tokenizer (timestamp-
                                                                          /+2.79% (CTR AUC/UAUC) and +1.14%/+3.23% (CVR AUC/UAUC),
aware fusion for multi-behavior sequences; Auto-Split for non-
                                                                          showing a predictable quality performance as model capacity grows.
sequential features) and a heuristic pyramid schedule that, at each
                                                                             In summary, unifying sequence modeling and feature interaction
layer, linearly shrinks the number of sequential query tokens from
                                                                          in a single Transformer yields more reliable and compute-efficient
1190 to 12 (OneTransS ) / from 1500 to 16 (OneTransL ). Concretely,
                                                                          improvements than scaling either component independently.
we linearly reduce the number of sequential query tokens across lay-
ers, rounding the token count at each layer to the nearest multiple
of 32, and set the top layer to match the number of non-sequential        4.3     RQ2: Design Choices via Ablation Study
tokens.                                                                   We perform an ablation study of the proposed OneTrans model
   Optimization and infrastructure. We use a dual-optimizer               to quantify the contribution of key design choices. The complete
strategy without weight decay: sparse embeddings are optimized            results are summarized in Table 3. We evaluate the following vari-
with Adagrad (ğ›½ 1 =0.1, ğ›½ 2 =1.0), and dense parameters with RM-          ants: Input variants: i) Replacing the Auto-Split Tokenizer with a
SProp (lr=0.005, alpha=0.99999,momentum=0). We apply stabiliza-           Group-wise Tokenizer (Row 1); ii) Using a timestamp-agnostic fusion
tion techniques commonly used in large-scale Transformer training,        strategy instead of the timestamp-aware sequence fusion (Row 2);
including Pre-Norm [29], and global grad-norm clipping [23]. The          iii) Removing [SEP] tokens in the timestamp-aware sequence fu-
per-GPU batch size is set to 2048 during training, with gradient          sion (Row 3); OneTrans block variants: i) Sharing a single set of
OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender   Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY


   Table 3: Impact of the choices of input design and OneTrans block design, using the OneTransS model as the reference.

                                                                                            CTR                 CVR (order)                 Efficiency
    Type                               Variant
                                                                                     AUC â†‘      UAUC â†‘       AUC â†‘      UAUC â†‘      Params (M)       TFLOPs
                                       Group-wise Tokenzier                          -0.10%      -0.30%      -0.12%      -0.10%                78         2.35
    Input                              Timestamp-agnostic Fusion                     -0.09%      -0.22%      -0.20%      -0.21%                91         2.64
                                       Timestamp-agnostic Fusion w/o Sep Tokens      -0.13%      -0.32%      -0.29%      -0.33%                91         2.62
                                       Shared parameters                             -0.15%      -0.29%      -0.14%     -0.29%                 24         2.64
    OneTrans Block                     Full attention                                +0.00%      +0.01%      -0.03%     +0.06%                 91         2.64
                                       w/o pyramid stack                             -0.05%      +0.06%      -0.04%     -0.42%                 92         8.08


                        0.65                                    Length (#Tokens)     outperform a shared projection, enabling better feature discrimi-
                                       T=2048                   Depth (#Layers)
                                                                Width (#Dim)         nation; 5) Causal and full attention perform similarly, but full
                        0.60                      L=10
                                                                                     attention disables standard optimizations such as KV caching; 6)
         CTR UAUC (%)




                                            L=8                                      Keeping full-length tokens at all layers provides no benefit: One-
                        0.55
                                T=1024                                               Trans effectively summarizes information into a small tail, so the
                        0.50                                             D=512       pyramid design can safely prune queries to save computation.
                                                                                     Moreover, under a fixed TFLOPs budget, the pyramid design sup-
                                                      D=384
                        0.45        L=6                                              ports close to 1.75Ã— longer sequences than a full-length design,
                               T=768 D=256                                           better exploiting gains from length extension.
                        0.40
                                   2              4       6         8          10    4.4      RQ3: Systems Efficiency
                                                   TFLOPs (T)
                                                                                     To quantify the optimizations in Section 3.5, we ablate them on an
                               (a) Trade-off: FLOPs vs. Î”UAUC                        unoptimized OneTransS baseline and report training/inference
                        1.60      OneTrans
                                  RankMixer
                                                                                     metrics in Table 4.
                                                                                         As shown in the table, (i) Pyramid stack reduces both training
                        1.20                                                         cost (runtime/memory) and serving overhead (p99 latency/memory)
         CTR UAUC (%)




                                                                                     by pruning sequential query tokens; (ii) cross-request KV caching
                        0.80                                                         removes redundant sequence computation, consistently improv-
                                                                                     ing runtime/latency and memory in both training and serving;
                        0.40                                                         (iii) FlashAttention yields substantial training gains with modest
                                                                                     serving improvements; and (iv) mixed precision with recompu-
                        0.00                                                         tation provides the largest serving gains (p99 latency and inference
                                                                                     memory), while also improving training efficiency.
                                       21             22            23                   These results demonstrate the effectiveness of LLM optimizations
                                              TFLOPs (T, log scale)                  for large-scale recommendation. Building on these results, we scale
                          (b) Scaling law: Î”UAUC vs. FLOPs (log)                     to OneTransL and show it maintains online efficiency comparable
                                                                                     to the much smaller DCNv2+DIN baseline (Table 5), highlighting
      Figure 3: Comparison of trade-off and scaling law.                             that a unified Transformer backbone enables direct adoption of
                                                                                     LLM optimizations.

Q/K/V and FFN parameters across all tokens, instead of assigning                     4.5      RQ4: Scaling-Law Validation
separate parameters to NS-tokens (Row 4); ii) Replacing causal at-                   We probe scaling laws for OneTrans along three axes: (1) length -
tention with full attention (Row 5); iii) Disabling the pyramid stack                input token sequence length, (2) depth - number of stacked blocks,
by keeping the full token sequence at all layers (Row 6).                            and (3) width - hidden-state dimensionality.
   In summary, the ablations show that 1) Auto-Split Tokenizer                          As shown in Fig. 3(a), increasing length yields the largest gains
provides a clear advantage over manually grouping non-sequential                     by introducing more behavioral evidence. Between depth and width,
features into tokens, indicating that allowing the model to automat-                 we observe a clear trade-off: increasing depth generally delivers
ically build non-sequential tokens is more effective than relying on                 larger performance improvements than simply widening width, as
human-defined feature grouping; 2) Timestamp-aware fusion                            deeper stacks extract higher-order interactions and richer abstrac-
beats intent-based ordering when timestamps exist, suggesting that                   tions. However, deeper models also increase serial computation,
temporal ordering should be prioritized over event impact; 3) Under                  whereas widening is more amenable to parallelism. Thus, choos-
timestamp-agnostic fusion, learnable [SEP] tokens help the model                     ing between depth and width should balance performance benefits
separate sequences; 4) Token-specific parameters for NS-tokens                       against system efficiency under the target hardware budget.
Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY                                                                                  Zhang et al.


                    Table 4: Impact of variants against the unoptimized OneTransS . Memory is peak GPU usage.

                                                                         Training                             Inference
                 Variant
                                                               Runtime (ms)    Memory (GB)         Latency (p99; ms)   Memory (GB)
                 Unoptimized OneTransS                                   407             53.13                 54.00           1.70
                 + Pyramid stack                                      âˆ’28.7%            âˆ’42.6%                âˆ’8.4%           âˆ’6.9%
                 + Cross-Request KV Caching                           âˆ’30.2%            âˆ’58.4%               âˆ’29.6%          âˆ’52.9%
                 + FlashAttention                                     âˆ’50.1%            âˆ’58.9%               âˆ’12.3%          âˆ’11.6%
                 + Mixed Precision with Recomputation                 âˆ’32.9%            âˆ’49.0%               âˆ’69.1%          âˆ’30.0%


Table 5: Key efficiency comparison between OneTransL and                       Table 6: Online A/B results: OneTransL (treatment) vs.
the DCNv2+DIN baseline.                                                        RankMixer+Transformer (control). Click/u, Order/u, GMV/u,
                                                                               are relative deltas (%). Latency is the relative end-to-end per-
   Metric                              DCNv2+DIN          OneTransL            impression change Î”% (lower is better). * denotes ğ‘ < 0.05,
                                                                               and ** for ğ‘ < 0.01
   TFLOPs                                    0.06            8.62
   Params (M)                                 10             330
                                                                                Scenario         click/u    order/u       gmv/u    Latency ( ğ‘99 ) â†“
   MFU                                       13.4            30.8
   Inference Latency (p99, ms)               13.6            13.2               Feeds       +7.737%**      +4.351%*    +5.685%*         âˆ’3.91%
   Training Memory (GB)                       20              32                Mall        +5.143%**      +2.577%**   +3.670%*         âˆ’3.26%
   Inference Memory (GB)                      1.8             0.8




    We further analyze scaling-law behavior by jointly widening                time from request arrival to response emission (Î”%; lower is bet-
and deepening OneTrans, and â€” for comparison â€” by scaling                      ter). As shown in Table 6, OneTransL delivers consistent gains. In
the RankMixer+Transformer baseline on the RankMixer side                       Feeds, it achieves +7.737% click/u, +4.3510% order/u, +5.6848%
till 1B; we then plot Î”UAUC versus training FLOPs on a log scale.              gmv/u, and âˆ’3.91% latency. In Mall, it achieves +5.143% click/u,
As shown in Fig. 3(b), OneTrans and RankMixer both exhibit                     +2.5772% order/u, +3.6696% gmv/u, and âˆ’3.26% latency. These
clear log-linear trends, but OneTrans shows a steeper slope, likely            results indicate that the unified modeling framework improves
because RankMixer-centric scaling lacks a unified backbone and                 business metrics while reducing serving time relative to a strong
its MoE-based expansion predominantly widens the FFN hidden                    non-unified baseline.
dimension. Together, these results suggest that OneTrans is more                  We further observe a +0.7478% increase in user Active Days and
parameter- and compute-efficient, offering favorable performanceâ€“              a significant improvement of +13.59% in cold-start product order/u,
compute trade-offs for industrial deployment.                                  highlighting the strong generalization capability of the proposed
    While we can deploy OneTransL under strict online p99 latency              model.
constraints, scaling substantially beyond this regime remains con-
strained by online efficiency, and we leave further systemâ€“model               5    Conclusion
co-optimizations to future work.
                                                                               We present OneTrans, a unified Transformer backbone for person-
                                                                               alized ranking to replace the conventional encodeâ€“thenâ€“interaction.
4.6     RQ5: Online A/B Tests                                                  A unified tokenizer converts both sequential and non-sequential at-
We assess the business impact of OneTrans in two large-scale                   tributes into one token sequence, and a unified Transformer block
industrial scenarios: (i) Feeds (home feeds), and (ii) Mall (the overall       jointly performs sequence modeling and feature interaction via
setting that includes Feeds and other sub-scenarios). Traffic is split         shared parameters for homogeneous (sequential) tokens and token-
at the user/account level with hashing and user-level randomization.           specific parameters for heterogeneous (non-sequential) tokens. To
Both the control and treatment models are trained and deployed with            make the unified stack efficient at scale, we adopt a pyramid sched-
the past 1.5 years of production data to ensure a fair comparison.             ule that progressively prunes sequential tokens and a cross-request
    Our prior production baseline, RankMixer+Transformer, serves               KV Caching that reuses user-side computation; the design further
as the control (â‰ˆ 100M neural-network parameters) and does not                 benefits from LLM-style systems optimizations (e.g., FlashAtten-
use sequence KV caching. The treatment deploys OneTransL with                  tion, mixed precision). Across large-scale evaluations, OneTrans
the serving optimizations described in Section 3.5.                            exhibits near log-linear performance gains as width/depth increase,
    We report user-level click/u, order/u, and gmv/u as relative               and delivers statistically significant business lifts while maintain-
deltas (Î”%) versus the RankMixer+Transformer control with two-                 ing production-grade latency. We believe this unified design offers
sided 95% CIs (user-level stratified bootstrap), and end-to-end                a practical way to scale recommender systems while reusing the
latency, measured as the relative change in p99 per-impression                 system optimizations that have powered recent LLM advances.
OneTrans: Unified Feature Interaction and Sequence Modeling with One Transformer in Industrial Recommender       Conference acronym â€™XX, June 03â€“05, 2018, Woodstock, NY


References                                                                                    3702â€“3712.
 [1] Zheng Chai, Qin Ren, Xijun Xiao, Huizhi Yang, Bo Han, Sijun Zhang, Di Chen,         [21] Qi Pi, Guorui Zhou, Yujing Zhang, Zhe Wang, Lejian Ren, Ying Fan, Xiaoqiang
     Hui Lu, Wenlin Zhao, Lele Yu, et al. 2025. LONGER: Scaling Up Long Sequence              Zhu, and Kun Gai. 2020. Search-based user interest modeling with lifelong
     Modeling in Industrial Recommenders. arXiv preprint arXiv:2505.04421 (2025).             sequential behavior data for click-through rate prediction. In Proceedings of the
 [2] Jianxin Chang, Chenbin Zhang, Yiqun Hui, Dewei Leng, Yanan Niu, Yang Song,               29th ACM International Conference on Information & Knowledge Management.
     and Kun Gai. 2023. Pepnet: Parameter and embedding personalized network for              2685â€“2692.
     infusing with personalized prior information. In Proceedings of the 29th ACM        [22] Jiarui Qin, Jiachen Zhu, Bo Chen, Zhirong Liu, Weiwen Liu, Ruiming Tang, Rui
     SIGKDD Conference on Knowledge Discovery and Data Mining. 3795â€“3804.                     Zhang, Yong Yu, and Weinan Zhang. 2022. Rankflow: Joint optimization of multi-
 [3] Yin-Wen Chang, Cho-Jui Hsieh, Kai-Wei Chang, Chih-Jen Lin, et al. 2010. Training         stage cascade ranking systems as flows. In Proceedings of the 45th International
                                                                                              ACM SIGIR Conference on Research and Development in Information Retrieval.
     and testing low-degree polynomial data mappings via linear svm. Journal of
                                                                                              814â€“824.
     Machine Learning Research 11, 4 (2010).
                                                                                         [23] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
 [4] Qiwei Chen, Huan Zhao, Wei Li, Pipei Huang, and Wenwu Ou. 2019. Be-
                                                                                              and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter
     havior Sequence Transformer for E-commerce Recommendation in Alibaba.
                                                                                              language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).
     arXiv:1905.06874 [cs.IR] https://arxiv.org/abs/1905.06874
                                                                                         [24] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,
 [5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,
                                                                                              and Jian Tang. 2019. AutoInt: Automatic Feature Interaction Learning via Self-
     Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, Rohan
                                                                                              Attentive Neural Networks. In Proceedings of the 28th ACM International Confer-
     Anil, Zakaria Haque, Lichan Hong, Vihan Jain, Xiaobing Liu, and Hemal Shah.
                                                                                              ence on Information and Knowledge Management (CIKM â€™19). ACM, 1161â€“1170.
     2016. Wide & Deep Learning for Recommender Systems. arXiv:1606.07792 [cs.LG]
                                                                                              doi:10.1145/3357384.3357925
     https://arxiv.org/abs/1606.07792
                                                                                         [25] Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang.
 [6] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks
                                                                                              2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Rep-
     for youtube recommendations. In Proceedings of the 10th ACM conference on
                                                                                              resentations from Transformer. arXiv:1904.06690 [cs.IR] https://arxiv.org/abs/
     recommender systems. 191â€“198.
                                                                                              1904.06690
 [7] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher RÃ©. 2022. Flashat-
                                                                                         [26] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang. 2017. Deep & Cross Network
     tention: Fast and memory-efficient exact attention with io-awareness. Advances
                                                                                              for Ad Click Predictions. arXiv:1708.05123 [cs.LG] https://arxiv.org/abs/1708.
     in neural information processing systems 35 (2022), 16344â€“16359.
                                                                                              05123
 [8] Tri Dao, Aleksander Thomas, Anima Anandkumar, Matei Zaharia, and Christo-
                                                                                         [27] Ruoxi Wang, Rakesh Shivanna, Derek Cheng, Sagar Jain, Dong Lin, Lichan Hong,
     pher Re. 2023. FlashAttention-2: Faster Attention with Better Parallelism and
                                                                                              and Ed Chi. 2021. DCN V2: Improved Deep & Cross Network and Practical
     Work Partitioning. arXiv preprint arXiv:2307.08691 (2023).
                                                                                              Lessons for Web-scale Learning to Rank Systems. In Proceedings of the Web
 [9] Yufei Feng, Fuyu Lv, Weichen Shen, Menghan Wang, Fei Sun, Yu Zhu, and Keping
                                                                                              Conference 2021 (WWW â€™21). ACM, 1785â€“1797. doi:10.1145/3442381.3450078
     Yang. 2019. Deep Session Interest Network for Click-Through Rate Prediction.
                                                                                         [28] Xue Xia, Pong Eksombatchai, Nikil Pancha, Dhruvil Deven Badani, Po-Wei Wang,
     arXiv:1905.06482 [cs.IR] https://arxiv.org/abs/1905.06482
                                                                                              Neng Gu, Saurabh Vishwas Joshi, Nazanin Farahpour, Zhiyuan Zhang, and An-
[10] Audrunas Gruslys, Remi Munos, Ivo Daniel, Oriol Vinyals, and Koray
                                                                                              drew Zhai. 2023. Transact: Transformer-based realtime user action model for
     Kavukcuoglu. 2016. Memory-Efficient Backpropagation through Time. In Ad-
                                                                                              recommendation at pinterest. In Proceedings of the 29th ACM SIGKDD Conference
     vances in Neural Information Processing Systems (NeurIPS).
                                                                                              on Knowledge Discovery and Data Mining. 5249â€“5259.
[11] Huan Gui, Ruoxi Wang, Ke Yin, Long Jin, Maciej Kula, Taibai Xu, Lichan Hong,
                                                                                         [29] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing,
     and Ed H. Chi. 2023. Hiformer: Heterogeneous Feature Interactions Learning
                                                                                              Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. 2020. On layer
     with Transformers for Recommender Systems. arXiv:2311.05884 [cs.IR] https:
                                                                                              normalization in the transformer architecture. In International conference on
     //arxiv.org/abs/2311.05884
                                                                                              machine learning. PMLR, 10524â€“10533.
[12] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He, and Zhenhua
                                                                                         [30] Zhichen Zeng, Xiaolong Liu, Mengyue Hang, Xiaoyi Liu, Qinghai Zhou, Chaofei
     Dong. 2018. DeepFM: An End-to-End Wide & Deep Learning Framework for
                                                                                              Yang, Yiqun Liu, Yichen Ruan, Laming Chen, Yuxin Chen, et al. 2024. Interformer:
     CTR Prediction. arXiv:1804.04950 [cs.IR] https://arxiv.org/abs/1804.04950
                                                                                              Towards effective heterogeneous interaction learning for click-through rate
[13] Junjie Huang, Jizheng Chen, Jianghao Lin, Jiarui Qin, Ziming Feng, Weinan
                                                                                              prediction. arXiv preprint arXiv:2411.09852 (2024).
     Zhang, and Yong Yu. 2024. A comprehensive survey on retrieval methods in
                                                                                         [31] Jiaqi Zhai, Lucy Liao, Xing Liu, Yueming Wang, Rui Li, Xuan Cao, Leon Gao, Zhao-
     recommender systems. arXiv preprint arXiv:2407.21022 (2024).
                                                                                              jie Gong, Fangda Gu, Michael He, et al. 2024. Actions speak louder than words:
[14] Wang-Cheng Kang and Julian McAuley. 2018. Self-Attentive Sequential Recom-
                                                                                              Trillion-parameter sequential transducers for generative recommendations. arXiv
     mendation. arXiv:1808.09781 [cs.IR] https://arxiv.org/abs/1808.09781
                                                                                              preprint arXiv:2402.17152 (2024).
[15] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
                                                                                         [32] Buyun Zhang, Liang Luo, Yuxin Chen, Jade Nie, Xi Liu, Daifeng Guo, Yanli Zhao,
     Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.
                                                                                              Shen Li, Yuchen Hao, Yantao Yao, et al. 2024. Wukong: Towards a scaling law for
     Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).
                                                                                              large-scale recommendation. arXiv preprint arXiv:2403.02545 (2024).
[16] Shichen Liu, Fei Xiao, Wenwu Ou, and Luo Si. 2017. Cascade ranking for opera-
                                                                                         [33] Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization.
     tional e-commerce search. In Proceedings of the 23rd ACM SIGKDD International
                                                                                              Advances in neural information processing systems 32 (2019).
     Conference on Knowledge Discovery and Data Mining. 1557â€“1565.
                                                                                         [34] Guorui Zhou, Na Mou, Ying Fan, Qi Pi, Weijie Bian, Chang Zhou, Xiaoqiang Zhu,
[17] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich
                                                                                              and Kun Gai. 2018. Deep Interest Evolution Network for Click-Through Rate
     Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
                                                                                              Prediction. arXiv:1809.03672 [stat.ML] https://arxiv.org/abs/1809.03672
     Venkatesh, et al. 2017. Mixed precision training. arXiv preprint arXiv:1710.03740
                                                                                         [35] Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Ying Fan, Han Zhu, Xiao Ma,
     (2017).
                                                                                              Yanghui Yan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep Interest Network for
[18] Paulius Micikevicius, Sharan Narang, Jonah Alben, Greg Diamos, Erich Elsen,
                                                                                              Click-Through Rate Prediction. arXiv:1706.06978 [stat.ML] https://arxiv.org/abs/
     David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
                                                                                              1706.06978
     Venkatesh, and Hao Wu. 2018. Mixed Precision Training. In International Confer-
                                                                                         [36] Han Zhu, Xiang Li, Pengye Zhang, Guozheng Li, Jie He, Han Li, and Kun Gai.
     ence on Learning Representations (ICLR).
                                                                                              2018. Learning tree-based deep model for recommender systems. In Proceedings
[19] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang,
                                                                                              of the 24th ACM SIGKDD international conference on knowledge discovery & data
     Narayanan Sundaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-
                                                                                              mining. 1079â€“1088.
     Jean Wu, Alisson G Azzolini, et al. 2019. Deep learning recommendation model
                                                                                         [37] Jie Zhu, Zhifang Fan, Xiaoxie Zhu, Yuchen Jiang, Hangyu Wang, Xintian Han,
     for personalization and recommendation systems. arXiv preprint arXiv:1906.00091
                                                                                              Haoran Ding, Xinmin Wang, Wenlin Zhao, Zhen Gong, Huizhi Yang, Zheng Chai,
     (2019).
                                                                                              Zhe Chen, Yuchao Zheng, Qiwei Chen, Feng Zhang, Xun Zhou, Peng Xu, Xiao
[20] Nikil Pancha, Andrew Zhai, Jure Leskovec, and Charles Rosenberg. 2022. Pinner-
                                                                                              Yang, Di Wu, and Zuotao Liu. 2025. RankMixer: Scaling Up Ranking Models in
     former: Sequence modeling for user representation at pinterest. In Proceedings
                                                                                              Industrial Recommenders. arXiv:2507.15551 [cs.IR] https://arxiv.org/abs/2507.
     of the 28th ACM SIGKDD conference on knowledge discovery and data mining.
                                                                                              15551
